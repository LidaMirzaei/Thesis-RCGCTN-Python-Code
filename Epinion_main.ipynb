{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eeda3c7-2190-4ef9-b691-1b3954fac844",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  # NumPy for numerical operations\n",
    "import pandas as pd  # Pandas for data manipulation and analysis\n",
    "from scipy.io import loadmat\n",
    "# Importing PyTorch and PyG modules, and the necessary libraries for this endS\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import ChebConv\n",
    "from torch_geometric.nn import RGCNConv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, f1_score, precision_score, recall_score\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a18ad0-0c19-42f4-8445-bf6cfc5114bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose one of following data path of Epinion, Enron, and Facebook\n",
    "epinion_path = 'Useable data/Epinion/download.tsv.epinions/epinions/epinion_dynamic.csv'\n",
    "\n",
    "# Read the trust dataset from the specified file\n",
    "df = pd.read_csv(epinion_path)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42fad1a9-448b-43f0-97c0-8617f11f39e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "21021d7e-f156-4d72-886b-1779d421ded4",
   "metadata": {},
   "source": [
    "# Proposed Confidence metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b519243c-9e5d-47e1-a3a0-f527cf243bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the combined confidence score\n",
    "numerator = 2 * df['jaccard_similarity'] * df['adamic_similarity']   # * df['common_neighbor']\n",
    "denominator = df['jaccard_similarity'] + df['adamic_similarity']     # + df['common_neighbor']\n",
    "\n",
    "# Handle potential division by zero\n",
    "df.loc[:, 'confidence'] = np.where(denominator != 0, (numerator / denominator), 0)\n",
    "\n",
    "# Normalize the confidence scores to the range [0, 1]\n",
    "df.loc[:, 'confidence'] = df['confidence'] / df['confidence'].max()\n",
    "\n",
    "# Print the DataFrame with the new columns\n",
    "df[['source_id', 'target_id', 'jaccard_similarity', 'adamic_similarity', 'confidence']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a827fcf-f8bd-40cf-b004-4bed377c9ae7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0b9c8e7a-c46d-49ab-89dd-97ea9875b94a",
   "metadata": {},
   "source": [
    "# Preparing data for test and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f6a2a6-cddb-4d7d-bb9e-6bc6a4565ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepares data for a PyTorch Geometric model\n",
    "\n",
    "# Assuming you have unique node IDs for source users and target users\n",
    "source_users = df['source_id'].unique()\n",
    "target_users = df['target_id'].unique()\n",
    "\n",
    "# Create a mapping of node IDs to indices\n",
    "node_to_index = {node: index for index, node in enumerate(set(source_users) | set(target_users))}\n",
    "\n",
    "# Map node IDs in the dataframe to indices\n",
    "df.loc[:,'source_index'] = df['source_id'].map(node_to_index)\n",
    "df.loc[:,'target_index'] = df['target_id'].map(node_to_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e29cdef-7a98-405c-afb2-87adb2ea9f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming df has columns 'source_id', 'target_id', 'timestamp', and others\n",
    "# Convert timestamps to Unix time\n",
    "df['timesdate'] = pd.to_datetime(df['timestamp'])\n",
    "df['timesdate'] = df['timesdate'].astype('int64') // 1e9\n",
    "df = df.sort_values(by='timestamp')\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b961f1-e54b-4ca3-b3f9-eb2761a988a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Concatenate polynomial features with the original features\n",
    "train_features = torch.tensor(train_df[['jaccard_similarity', 'adamic_similarity', 'confidence']].values, dtype=torch.float)\n",
    "test_features = torch.tensor(test_df[['jaccard_similarity', 'adamic_similarity', 'confidence']].values, dtype=torch.float)\n",
    "\n",
    "\n",
    "# Convert list of NumPy arrays to a single NumPy array\n",
    "edge_index_train_np = np.array([train_df['source_index'].values, train_df['target_index'].values])\n",
    "edge_index_test_np = np.array([test_df['source_index'].values, test_df['target_index'].values])\n",
    "# Convert the NumPy arrays to PyTorch tensors\n",
    "edge_index_train = torch.tensor(edge_index_train_np, dtype=torch.long)\n",
    "edge_index_test = torch.tensor(edge_index_test_np, dtype=torch.long)\n",
    "\n",
    "# 'timestamp', 'normalized_time'\n",
    "edge_attr_train = torch.tensor(train_df['normalized_time'].values, dtype=torch.float).view(-1, 1)\n",
    "edge_attr_test = torch.tensor(test_df['normalized_time'].values, dtype=torch.float).view(-1, 1)\n",
    "\n",
    "train_labels = torch.tensor(train_df['relation_label'].values, dtype=torch.long)\n",
    "test_labels = torch.tensor(test_df['relation_label'].values, dtype=torch.long)\n",
    "\n",
    "train_data = Data(x=train_features, edge_index=edge_index_train, edge_attr=edge_attr_train, y=train_labels)\n",
    "test_data = Data(x=test_features, edge_index=edge_index_test, edge_attr=edge_attr_test, y=test_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55cac21-feba-45da-8b2f-68ddc771df3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c0ea4377-b639-41b1-b350-b2ace42e9fcc",
   "metadata": {},
   "source": [
    "# Proposed Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7906c51-ad2d-43e1-8973-daef86ee3028",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.001\n",
    "N_Epo = 200\n",
    "hidden_dim_ae = 64\n",
    "hidden_dim_rgcn = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3c67d5-5243-43c3-a06d-ee8e80869b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the autoencoder model\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, activation_fn=nn.ReLU()):\n",
    "        super(Autoencoder, self).__init__()\n",
    "\n",
    "        # Encoder layers\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            activation_fn\n",
    "        )\n",
    "\n",
    "        # Decoder layers\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, input_dim),\n",
    "            nn.Sigmoid()  # Sigmoid activation for reconstruction\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return encoded, decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19e0e5b-8ecd-41c7-80a6-543fe20a5a43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e488f60e-3f37-42ab-b0db-41854da9aa56",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TGCNConv(ChebConv):\n",
    "    def __init__(self, in_channels, out_channels, K=2, normalization='sym', bias=True, **kwargs):\n",
    "        super(TGCNConv, self).__init__(in_channels, out_channels, K, normalization, bias, **kwargs)\n",
    "        self.lin = nn.Linear(in_channels, out_channels)\n",
    "        # Initialize weights for the ChebConv layers\n",
    "        nn.init.kaiming_uniform_(self.lin.weight, mode='fan_in', nonlinearity='relu')\n",
    "        nn.init.zeros_(self.lin.bias)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        # L2 normalization on input features\n",
    "        x = F.normalize(x, p=2, dim=1)\n",
    "        edge_attr = edge_attr.view(-1, 1)\n",
    "        return super(TGCNConv, self).forward(x, edge_index, edge_attr)\n",
    "\n",
    "class RealationPrediction_Model(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim_ae, hidden_dim_rgcn, output_dim, num_relations, num_bases, num_time_embeddings, poly_degree, dropout_rate=0.5):\n",
    "        super(RealationPrediction_Model, self).__init__()\n",
    "        self.hidden_dim_ae = hidden_dim_ae\n",
    "        self.hidden_dim_rgcn = hidden_dim_rgcn\n",
    "        self.autoencoder = Autoencoder(input_dim * poly_degree, hidden_dim_ae)\n",
    "        self.time_embedding = nn.Embedding(num_time_embeddings, hidden_dim_ae)\n",
    "        \n",
    "        self.rgcn = TGCNConv(hidden_dim_ae, hidden_dim_rgcn, K=2, normalization='sym')\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)\n",
    "        self.rgcn1 = TGCNConv(hidden_dim_rgcn, hidden_dim_rgcn, K=2, normalization='sym')\n",
    "        self.dropout2 = nn.Dropout(dropout_rate)\n",
    "\n",
    "        self.rgcn2 = RGCNConv(hidden_dim_rgcn, hidden_dim_rgcn, num_relations, num_bases=2)\n",
    "        self.linear = nn.Linear(hidden_dim_rgcn, output_dim)\n",
    "\n",
    "        self.poly_degree = poly_degree\n",
    "        \n",
    "        #nn.init.kaiming_uniform_(self.linear.weight, mode='fan_in', nonlinearity='relu')\n",
    "        #nn.init.zeros_(self.linear.bias)\n",
    "\n",
    "    \n",
    "    def generate_polynomial_features(self, x):\n",
    "        poly_features = [x[:, i] ** d for d in range(1, self.poly_degree + 1) for i in range(x.shape[1])]\n",
    "        return torch.stack(poly_features, dim=1)\n",
    "\n",
    "    \n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_attr, edge_type = data.x, data.edge_index, data.edge_attr, data.y\n",
    "        \n",
    "        poly_features = self.generate_polynomial_features(x)\n",
    "        encoded, _ = self.autoencoder(poly_features)\n",
    "        time_embedding = self.time_embedding(edge_attr.long())\n",
    "        if time_embedding.dim() == 3:\n",
    "            time_embedding = time_embedding.squeeze(1)\n",
    "            \n",
    "        x_with_temporal = torch.cat([encoded, time_embedding], dim=1)\n",
    "        x_with_temporal = x_with_temporal.view(-1, self.hidden_dim_ae * 2)\n",
    "\n",
    "        x = F.relu(self.rgcn(encoded, edge_index, edge_attr))\n",
    "        x = self.dropout1(x)\n",
    "        x = F.relu(self.rgcn1(x, edge_index, edge_attr))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.rgcn2(x, edge_index, edge_type)\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Initialize the proposed method\n",
    "input_dim_ae = train_data.x.shape[1]\n",
    "num_time_embeddings = 10  # Example value\n",
    "num_relations = df['relation_label'].nunique()  # Example value\n",
    "output_dim = 1  # Binary classification (trust/distrust)\n",
    "\n",
    "proposed_model = RealationPrediction_Model(\n",
    "    input_dim_ae,\n",
    "    hidden_dim_ae,\n",
    "    hidden_dim_rgcn,\n",
    "    output_dim,\n",
    "    num_relations,\n",
    "    num_bases=2,\n",
    "    num_time_embeddings=num_time_embeddings,\n",
    "    poly_degree=2\n",
    ")\n",
    "\n",
    "# Define loss function and optimizer with weight decay\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(proposed_model.parameters(), lr, weight_decay=1e-4)\n",
    "\n",
    "start_time = time.time()\n",
    "# Train the model with dropout and weight decay\n",
    "proposed_model.train()\n",
    "for epoch in range(N_Epo):\n",
    "    optimizer.zero_grad()\n",
    "    out = proposed_model(train_data)\n",
    "\n",
    "    target = train_data.y.float().view(-1, 1)\n",
    "\n",
    "    loss = criterion(out, target)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch {epoch}, Loss: {loss.item()}')\n",
    "\n",
    "end_time = time.time()\n",
    "# Evaluation on test data\n",
    "proposed_model.eval()\n",
    "with torch.no_grad():\n",
    "    proposed_model_pred = proposed_model(test_data)\n",
    "\n",
    "\n",
    "\n",
    "runtime = end_time - start_time\n",
    "print(\"Runtime:\", runtime, \"seconds\")\n",
    "ourmodel_time = runtime\n",
    "\n",
    "proposed_model_probabilities = torch.sigmoid(torch.sigmoid(proposed_model_pred))\n",
    "\n",
    "# Compute AUC\n",
    "ourmodel_auc = roc_auc_score(test_labels.numpy(), proposed_model_pred.numpy())\n",
    "\n",
    "\n",
    "# Get the predicted rankings for each edge\n",
    "indices, _ = torch.sort(proposed_model_probabilities, descending=True)\n",
    "ranks = torch.zeros_like(indices, dtype=torch.float)  # Initialize ranks\n",
    "\n",
    "# Loop through the sorted indices to calculate ranks\n",
    "for i, idx in enumerate(indices):\n",
    "    ranks[i] = idx + 1  # Adjust rank starting from 1\n",
    "\n",
    "# Number of unlabeled links\n",
    "num_positive_links = torch.sum(test_labels == 1)\n",
    "num_negative_links = torch.sum(test_labels == 0)\n",
    "denominator = num_positive_links / (num_positive_links - num_negative_links)\n",
    "\n",
    "# Calculate RS for each positive link\n",
    "positive_indices = test_labels == 1\n",
    "positive_ranks = ranks[positive_indices]\n",
    "RSe_values = positive_ranks / denominator\n",
    "ourmodel_RS = torch.mean(RSe_values).item()\n",
    "\n",
    "\n",
    "print(f'AUC: {ourmodel_auc}, RS: {ourmodel_RS}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbcca842-62f9-416f-af0e-bb80185e0549",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8514eaa3-a099-43a7-af44-8caeabe03fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrix in sklearn\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# actual values\n",
    "actual = test_labels.numpy()\n",
    "# predicted values\n",
    "predicted = (proposed_model_pred >= torch.mean(proposed_model_pred)).int()\n",
    "\n",
    "\n",
    "# confusion matrix\n",
    "ourmodel_matrix = confusion_matrix(actual,predicted, labels=[1,0])\n",
    "print('Confusion matrix : \\n',ourmodel_matrix)\n",
    "\n",
    "# outcome values order in sklearn\n",
    "tp, fn, fp, tn = confusion_matrix(actual,predicted,labels=[1,0]).reshape(-1)\n",
    "print('Outcome values : \\n', tp, fn, fp, tn)\n",
    "\n",
    "# classification report for precision, recall f1-score and accuracy\n",
    "ourmodel_matrix = classification_report(actual,predicted,labels=[1,0])\n",
    "print('Classification report : \\n',ourmodel_matrix)\n",
    "\n",
    "# Calculate Accuracy, Precision, Recall, F1\n",
    "ourmodel_accuracy = accuracy_score(test_labels.numpy(), predicted.numpy())\n",
    "ourmodel_precision = precision_score(test_labels.numpy(), predicted.numpy())\n",
    "ourmodel_recall = recall_score(test_labels.numpy(), predicted.numpy())\n",
    "ourmodel_f1 = f1_score(test_labels.numpy(), predicted.numpy())\n",
    "print(f'Accuracy: {ourmodel_accuracy}, Precision: {ourmodel_precision}, Recall: {ourmodel_recall}, F1: {ourmodel_f1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6ea87f-8f4c-4a1b-91ba-35cc1cd4f038",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f983eba-081d-494d-9811-d4ebd742d3a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4e132b73-1a19-405a-a9ad-8623b74acdb2",
   "metadata": {},
   "source": [
    "# Compared Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a432d88-253d-4e78-a60d-6fed8f6bda10",
   "metadata": {},
   "source": [
    "## Other variant of our proposed "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ed4e50-ce8f-451b-9537-865b7c68e521",
   "metadata": {},
   "source": [
    "### With out PFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b68fee-5647-4d36-b431-050887dfbdbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = torch.tensor(train_df[['jaccard_similarity', 'adamic_similarity', 'confidence']].values, dtype=torch.float)\n",
    "test_features = torch.tensor(test_df[['jaccard_similarity', 'adamic_similarity', 'confidence']].values, dtype=torch.float)\n",
    "edge_index_train_np = np.array([train_df['source_index'].values, train_df['target_index'].values])\n",
    "edge_index_test_np = np.array([test_df['source_index'].values, test_df['target_index'].values])\n",
    "edge_index_train = torch.tensor(edge_index_train_np, dtype=torch.long)\n",
    "edge_index_test = torch.tensor(edge_index_test_np, dtype=torch.long)\n",
    "edge_attr_train = torch.tensor(train_df['normalized_time'].values, dtype=torch.float).view(-1, 1)\n",
    "edge_attr_test = torch.tensor(test_df['normalized_time'].values, dtype=torch.float).view(-1, 1)\n",
    "train_labels = torch.tensor(train_df['relation_label'].values, dtype=torch.long)\n",
    "test_labels = torch.tensor(test_df['relation_label'].values, dtype=torch.long)\n",
    "train_data = Data(x=train_features, edge_index=edge_index_train, edge_attr=edge_attr_train, y=train_labels)\n",
    "test_data = Data(x=test_features, edge_index=edge_index_test, edge_attr=edge_attr_test, y=test_labels)\n",
    "\n",
    "# Define the autoencoder model\n",
    "class Autoencoder2(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, activation_fn=nn.ReLU()):\n",
    "        super(Autoencoder2, self).__init__()\n",
    "\n",
    "        # Encoder layers\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            activation_fn\n",
    "        )\n",
    "\n",
    "        # Decoder layers\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, input_dim),\n",
    "            nn.Sigmoid()  # Sigmoid activation for reconstruction\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return encoded, decoded\n",
    "        \n",
    "class TGCNConv2(ChebConv):\n",
    "    def __init__(self, in_channels, out_channels, K=2, normalization='sym', bias=True, **kwargs):\n",
    "        super(TGCNConv2, self).__init__(in_channels, out_channels, K, normalization, bias, **kwargs)\n",
    "        self.lin = nn.Linear(in_channels, out_channels)\n",
    "        # Initialize weights for the ChebConv layers\n",
    "        nn.init.kaiming_uniform_(self.lin.weight, mode='fan_in', nonlinearity='relu')\n",
    "        nn.init.zeros_(self.lin.bias)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        # L2 normalization on input features\n",
    "        x = F.normalize(x, p=2, dim=1)\n",
    "        edge_attr = edge_attr.view(-1, 1)\n",
    "        return super(TGCNConv2, self).forward(x, edge_index, edge_attr)\n",
    "        \n",
    "class no_PFs_Model(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim_ae, hidden_dim_rgcn, output_dim, num_relations, num_bases, num_time_embeddings, dropout_rate=0.5):\n",
    "        super(no_PFs_Model, self).__init__()\n",
    "        self.hidden_dim_ae = hidden_dim_ae\n",
    "        self.hidden_dim_rgcn = hidden_dim_rgcn\n",
    "        self.autoencoder = Autoencoder2(input_dim, hidden_dim_ae)\n",
    "        self.time_embedding = nn.Embedding(num_time_embeddings, hidden_dim_ae)\n",
    "        self.rgcn1 = TGCNConv2(hidden_dim_ae + hidden_dim_ae, hidden_dim_rgcn, K=2, normalization='sym')\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)\n",
    "        self.rgcn2 = TGCNConv2(hidden_dim_rgcn, hidden_dim_rgcn, K=2, normalization='sym')\n",
    "        self.dropout2 = nn.Dropout(dropout_rate)\n",
    "        self.linear = nn.Linear(hidden_dim_rgcn, output_dim)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_attr = data.x, data.edge_index, data.edge_attr\n",
    "        \n",
    "        encoded, _ = self.autoencoder(x)\n",
    "        time_embedding = self.time_embedding(edge_attr.long())\n",
    "\n",
    "        if time_embedding.dim() == 3:\n",
    "            time_embedding = time_embedding.squeeze(1)\n",
    "\n",
    "        x_with_temporal = torch.cat([encoded, time_embedding], dim=1)\n",
    "        x_with_temporal = x_with_temporal.view(-1, self.hidden_dim_ae * 2)\n",
    "\n",
    "        x = F.relu(self.rgcn1(x_with_temporal, edge_index, edge_attr))\n",
    "        x = self.dropout1(x)\n",
    "        x = F.relu(self.rgcn2(x, edge_index, edge_attr))\n",
    "        x = self.dropout2(x)\n",
    "        x = torch.sigmoid(self.linear(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "# Initialize the proposed method\n",
    "input_dim_ae = train_data.x.shape[1]\n",
    "num_time_embeddings = 10  # Example value\n",
    "num_relations = df['relation_label'].nunique()  # Example value\n",
    "output_dim = 1  # Binary classification (trust/distrust)\n",
    "\n",
    "no_PFs_model = no_PFs_Model(\n",
    "    input_dim_ae,\n",
    "    hidden_dim_ae,\n",
    "    hidden_dim_rgcn,\n",
    "    output_dim,\n",
    "    num_relations,\n",
    "    num_bases=2,\n",
    "    num_time_embeddings=num_time_embeddings\n",
    ")\n",
    "\n",
    "# Define loss function and optimizer with weight decay\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(no_PFs_model.parameters(), lr, weight_decay=1e-4)\n",
    "\n",
    "start_time = time.time()\n",
    "# Train the model with dropout and weight decay\n",
    "no_PFs_model.train()\n",
    "for epoch in range(N_Epo):\n",
    "    optimizer.zero_grad()\n",
    "    out = no_PFs_model(train_data)\n",
    "\n",
    "    target = train_data.y.float().view(-1, 1)\n",
    "\n",
    "    loss = criterion(out, target)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch {epoch}, Loss: {loss.item()}')\n",
    "\n",
    "end_time = time.time()\n",
    "# Evaluation on test data\n",
    "no_PFs_model.eval()\n",
    "with torch.no_grad():\n",
    "    no_PFs_model_pred = no_PFs_model(test_data)\n",
    "\n",
    "\n",
    "\n",
    "no_PFs_pred_probabilities = torch.sigmoid(no_PFs_model_pred)\n",
    "\n",
    "\n",
    "runtime = end_time - start_time\n",
    "print(\"Runtime:\", runtime, \"seconds\")\n",
    "no_PFs_time = runtime\n",
    "\n",
    "\n",
    "# Compute AUC\n",
    "# Compute AUC\n",
    "no_PFs_auc = roc_auc_score(test_labels.numpy(), no_PFs_pred_probabilities.numpy())\n",
    "\n",
    "# Get the predicted rankings for each edge\n",
    "indices, _ = torch.sort(no_PFs_pred_probabilities, descending=True)\n",
    "ranks = torch.zeros_like(indices, dtype=torch.float)  # Initialize ranks\n",
    "\n",
    "# Loop through the sorted indices to calculate ranks\n",
    "for i, idx in enumerate(indices):\n",
    "    ranks[i] = idx + 1  # Adjust rank starting from 1\n",
    "\n",
    "# Number of unlabeled links\n",
    "num_positive_links = torch.sum(test_labels == 1)\n",
    "num_negative_links = torch.sum(test_labels == 0)\n",
    "denominator = num_positive_links / (num_positive_links - num_negative_links)\n",
    "\n",
    "# Calculate RS for each positive link\n",
    "positive_indices = test_labels == 1\n",
    "positive_ranks = ranks[positive_indices]\n",
    "RSe_values = positive_ranks / denominator\n",
    "no_PFs_RS = torch.mean(RSe_values).item()\n",
    "\n",
    "print('Evaluation results for proposed without polynomial features:')\n",
    "print(f'AUC: {no_PFs_auc}, RS: {no_PFs_RS}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92100fa7-8e70-4c8f-8d6b-09d55ff7f117",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74480b7b-1fe0-4c99-88ac-576d3b0205b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# actual values\n",
    "actual = test_labels.numpy()\n",
    "# predicted values\n",
    "predicted = (no_PFs_pred_probabilities > torch.mean(no_PFs_pred_probabilities)).int()\n",
    "\n",
    "\n",
    "# confusion matrix\n",
    "no_PFs_matrix = confusion_matrix(actual,predicted, labels=[1,0])\n",
    "print('Confusion matrix : \\n',no_PFs_matrix)\n",
    "\n",
    "# outcome values order in sklearn\n",
    "tp, fn, fp, tn = confusion_matrix(actual,predicted,labels=[1,0]).reshape(-1)\n",
    "print('Outcome values : \\n', tp, fn, fp, tn)\n",
    "\n",
    "# classification report for precision, recall f1-score and accuracy\n",
    "no_PFs_matrix = classification_report(actual,predicted,labels=[1,0])\n",
    "print('Classification report : \\n',no_PFs_matrix)\n",
    "\n",
    "# Calculate Accuracy, Precision, Recall, F1\n",
    "no_PFs_accuracy = accuracy_score(test_labels.numpy(), predicted.numpy())\n",
    "no_PFs_precision = precision_score(test_labels.numpy(), predicted.numpy())\n",
    "no_PFs_recall = recall_score(test_labels.numpy(), predicted.numpy())\n",
    "no_PFs_f1 = f1_score(test_labels.numpy(), predicted.numpy())\n",
    "print(f'Accuracy: {no_PFs_accuracy}, Precision: {no_PFs_precision}, Recall: {no_PFs_recall}, F1: {no_PFs_f1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2913a565-b208-4490-9290-65c8cc1e1459",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "691b0897-7d9c-41f4-b35f-f8b81e358580",
   "metadata": {},
   "source": [
    "### With out confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df97470-1937-42ba-9d74-959b98c4cf75",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = torch.tensor(train_df[['jaccard_similarity','adamic_similarity']].values, dtype=torch.float)\n",
    "test_features = torch.tensor(test_df[['jaccard_similarity','adamic_similarity']].values, dtype=torch.float)\n",
    "edge_index_train_np = np.array([train_df['source_index'].values, train_df['target_index'].values])\n",
    "edge_index_test_np = np.array([test_df['source_index'].values, test_df['target_index'].values])\n",
    "edge_index_train = torch.tensor(edge_index_train_np, dtype=torch.long)\n",
    "edge_index_test = torch.tensor(edge_index_test_np, dtype=torch.long)\n",
    "edge_attr_train = torch.tensor(train_df['normalized_time'].values, dtype=torch.float).view(-1, 1)\n",
    "edge_attr_test = torch.tensor(test_df['normalized_time'].values, dtype=torch.float).view(-1, 1)\n",
    "train_labels = torch.tensor(train_df['relation_label'].values, dtype=torch.long)\n",
    "test_labels = torch.tensor(test_df['relation_label'].values, dtype=torch.long)\n",
    "train_data = Data(x=train_features, edge_index=edge_index_train, edge_attr=edge_attr_train, y=train_labels)\n",
    "test_data = Data(x=test_features, edge_index=edge_index_test, edge_attr=edge_attr_test, y=test_labels)\n",
    "# Define the autoencoder model\n",
    "class Autoencoder3(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, activation_fn=nn.ReLU()):\n",
    "        super(Autoencoder3, self).__init__()\n",
    "\n",
    "        # Encoder layers\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            activation_fn\n",
    "        )\n",
    "\n",
    "        # Decoder layers\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, input_dim),\n",
    "            nn.Sigmoid()  # Sigmoid activation for reconstruction\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return encoded, decoded\n",
    "        \n",
    "class TGCNConv3(ChebConv):\n",
    "    def __init__(self, in_channels, out_channels, K=2, normalization='sym', bias=True, **kwargs):\n",
    "        super(TGCNConv3, self).__init__(in_channels, out_channels, K, normalization, bias, **kwargs)\n",
    "        self.lin = nn.Linear(in_channels, out_channels)\n",
    "        # Initialize weights for the ChebConv layers\n",
    "        nn.init.kaiming_uniform_(self.lin.weight, mode='fan_in', nonlinearity='relu')\n",
    "        nn.init.zeros_(self.lin.bias)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        # L2 normalization on input features\n",
    "        x = F.normalize(x, p=2, dim=1)\n",
    "        edge_attr = edge_attr.view(-1, 1)\n",
    "        return super(TGCNConv3, self).forward(x, edge_index, edge_attr)\n",
    "\n",
    "\n",
    "class RealationPrediction_Model_2(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim_ae, hidden_dim_rgcn, output_dim, num_relations, num_bases, num_time_embeddings, poly_degree, dropout_rate=0.5):\n",
    "        super(RealationPrediction_Model_2, self).__init__()\n",
    "        self.hidden_dim_ae = hidden_dim_ae\n",
    "        self.hidden_dim_rgcn = hidden_dim_rgcn\n",
    "        self.autoencoder = Autoencoder3(input_dim * poly_degree, hidden_dim_ae)\n",
    "        self.time_embedding = nn.Embedding(num_time_embeddings, hidden_dim_ae)\n",
    "        self.rgcn1 = TGCNConv3(hidden_dim_ae + hidden_dim_ae, hidden_dim_rgcn, K=2, normalization='sym')\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)\n",
    "        self.rgcn2 = TGCNConv3(hidden_dim_rgcn, hidden_dim_rgcn, K=2, normalization='sym')\n",
    "        self.dropout2 = nn.Dropout(dropout_rate)\n",
    "        self.linear = nn.Linear(hidden_dim_rgcn, output_dim)\n",
    "        self.poly_degree = poly_degree\n",
    "        \n",
    "        # Initialize weights for the linear layer\n",
    "        nn.init.kaiming_uniform_(self.linear.weight, mode='fan_in', nonlinearity='relu')\n",
    "        nn.init.zeros_(self.linear.bias)\n",
    "\n",
    "    def generate_polynomial_features(self, x):\n",
    "        poly_features = [x[:, i] ** d for d in range(1, self.poly_degree + 1) for i in range(x.shape[1])]\n",
    "        return torch.stack(poly_features, dim=1)\n",
    "\n",
    "    \n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_attr = data.x, data.edge_index, data.edge_attr\n",
    "        \n",
    "        poly_features = self.generate_polynomial_features(x)\n",
    "        encoded, _ = self.autoencoder(poly_features)\n",
    "        time_embedding = self.time_embedding(edge_attr.long())\n",
    "\n",
    "        if time_embedding.dim() == 3:\n",
    "            time_embedding = time_embedding.squeeze(1)\n",
    "\n",
    "        x_with_temporal = torch.cat([encoded, time_embedding], dim=1)\n",
    "        x_with_temporal = x_with_temporal.view(-1, self.hidden_dim_ae * 2)\n",
    "\n",
    "        x = F.relu(self.rgcn1(x_with_temporal, edge_index, edge_attr))\n",
    "        x = self.dropout1(x)\n",
    "        x = F.relu(self.rgcn2(x, edge_index, edge_attr))\n",
    "        x = self.dropout2(x)\n",
    "        x = torch.sigmoid(self.linear(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "# Initialize the proposed method\n",
    "input_dim_ae = train_data.x.shape[1]\n",
    "num_time_embeddings = 10  # Example value\n",
    "num_relations = df['relation_label'].nunique()  # Example value\n",
    "output_dim = 1  # Binary classification (trust/distrust)\n",
    "\n",
    "no_confidence_model = RealationPrediction_Model_2(\n",
    "    input_dim_ae,\n",
    "    hidden_dim_ae,\n",
    "    hidden_dim_rgcn,\n",
    "    output_dim,\n",
    "    num_relations,\n",
    "    num_bases=2,\n",
    "    num_time_embeddings=num_time_embeddings,\n",
    "    poly_degree=2\n",
    ")\n",
    "\n",
    "# Define loss function and optimizer with weight decay\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(no_confidence_model.parameters(), lr, weight_decay=1e-4)\n",
    "\n",
    "start_time = time.time()\n",
    "# Train the model with dropout and weight decay\n",
    "no_confidence_model.train()\n",
    "for epoch in range(N_Epo):\n",
    "    optimizer.zero_grad()\n",
    "    out = no_confidence_model(train_data)\n",
    "\n",
    "    target = train_data.y.float().view(-1, 1)\n",
    "\n",
    "    loss = criterion(out, target)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch {epoch}, Loss: {loss.item()}')\n",
    "\n",
    "end_time = time.time()\n",
    "# Evaluation on test data\n",
    "no_confidence_model.eval()\n",
    "with torch.no_grad():\n",
    "    no_confidence_model_pred = no_confidence_model(test_data)\n",
    "\n",
    "\n",
    "no_confidence_probabilities =  torch.sigmoid(torch.sigmoid(torch.sigmoid(no_confidence_model_pred)))\n",
    "\n",
    "runtime = end_time - start_time\n",
    "print(\"Runtime:\", runtime, \"seconds\")\n",
    "no_confidence_time = runtime\n",
    "\n",
    "# Compute AUC\n",
    "no_confidence_auc = roc_auc_score(test_labels.numpy(), no_confidence_probabilities.numpy())\n",
    "\n",
    "# Get the predicted rankings for each edge\n",
    "indices, _ = torch.sort(no_confidence_model_pred, descending=True)\n",
    "ranks = torch.zeros_like(indices, dtype=torch.float)  # Initialize ranks\n",
    "\n",
    "# Loop through the sorted indices to calculate ranks\n",
    "for i, idx in enumerate(indices):\n",
    "    ranks[i] = idx + 1  # Adjust rank starting from 1\n",
    "\n",
    "# Number of unlabeled links\n",
    "num_positive_links = torch.sum(test_labels == 1)\n",
    "num_negative_links = torch.sum(test_labels == 0)\n",
    "denominator = num_positive_links / (num_positive_links - num_negative_links)\n",
    "\n",
    "# Calculate RS for each positive link\n",
    "positive_indices = test_labels == 1\n",
    "positive_ranks = ranks[positive_indices]\n",
    "RSe_values = positive_ranks / denominator\n",
    "no_confidence_RS = torch.mean(RSe_values).item()\n",
    "\n",
    "print('Evaluation results for proposed without confidence:')\n",
    "print(f'AUC: {no_confidence_auc}, RS: {no_confidence_RS}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6e5452-9104-4bb8-ad90-184b48d1f274",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af60df99-fe15-4b27-86a5-64d05c178ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# actual values\n",
    "actual = test_labels.numpy()\n",
    "# predicted values\n",
    "predicted = (no_confidence_probabilities >= torch.mean(no_confidence_probabilities)).int()\n",
    "\n",
    "\n",
    "# confusion matrix\n",
    "no_confidence_matrix = confusion_matrix(actual,predicted, labels=[1,0])\n",
    "print('Confusion matrix : \\n',no_confidence_matrix)\n",
    "\n",
    "# outcome values order in sklearn\n",
    "tp, fn, fp, tn = confusion_matrix(actual,predicted,labels=[1,0]).reshape(-1)\n",
    "print('Outcome values : \\n', tp, fn, fp, tn)\n",
    "\n",
    "# classification report for precision, recall f1-score and accuracy\n",
    "no_confidence_matrix = classification_report(actual,predicted,labels=[1,0])\n",
    "print('Classification report : \\n',no_confidence_matrix)\n",
    "\n",
    "# Calculate Accuracy, Precision, Recall, F1\n",
    "no_confidence_accuracy = accuracy_score(test_labels.numpy(), predicted.numpy())\n",
    "no_confidence_precision = precision_score(test_labels.numpy(), predicted.numpy())\n",
    "no_confidence_recall = recall_score(test_labels.numpy(), predicted.numpy())\n",
    "no_confidence_f1 = f1_score(test_labels.numpy(), predicted.numpy())\n",
    "print(f'Accuracy: {no_confidence_accuracy}, Precision: {no_confidence_precision}, Recall: {no_confidence_recall}, F1: {no_confidence_f1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4296a3ec-7b58-4023-adda-c9570db245cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a2c67cbb-eeb6-4070-80a5-0f67cef3218b",
   "metadata": {},
   "source": [
    "### With out both PFs and Confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5b3c93-e504-442e-9033-f5f2b83c4eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = torch.tensor(train_df[['jaccard_similarity','adamic_similarity']].values, dtype=torch.float)\n",
    "test_features = torch.tensor(test_df[['jaccard_similarity','adamic_similarity']].values, dtype=torch.float)\n",
    "edge_index_train_np = np.array([train_df['source_index'].values, train_df['target_index'].values])\n",
    "edge_index_test_np = np.array([test_df['source_index'].values, test_df['target_index'].values])\n",
    "edge_index_train = torch.tensor(edge_index_train_np, dtype=torch.long)\n",
    "edge_index_test = torch.tensor(edge_index_test_np, dtype=torch.long)\n",
    "edge_attr_train = torch.tensor(train_df['normalized_time'].values, dtype=torch.float).view(-1, 1)\n",
    "edge_attr_test = torch.tensor(test_df['normalized_time'].values, dtype=torch.float).view(-1, 1)\n",
    "train_labels = torch.tensor(train_df['relation_label'].values, dtype=torch.long)\n",
    "test_labels = torch.tensor(test_df['relation_label'].values, dtype=torch.long)\n",
    "train_data = Data(x=train_features, edge_index=edge_index_train, edge_attr=edge_attr_train, y=train_labels)\n",
    "test_data = Data(x=test_features, edge_index=edge_index_test, edge_attr=edge_attr_test, y=test_labels)\n",
    "\n",
    "# Define the autoencoder model\n",
    "class Autoencoder4(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, activation_fn=nn.ReLU()):\n",
    "        super(Autoencoder4, self).__init__()\n",
    "\n",
    "        # Encoder layers\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            activation_fn\n",
    "        )\n",
    "\n",
    "        # Decoder layers\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, input_dim),\n",
    "            nn.Sigmoid()  # Sigmoid activation for reconstruction\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return encoded, decoded\n",
    "        \n",
    "class TGCNConv4(ChebConv):\n",
    "    def __init__(self, in_channels, out_channels, K=2, normalization='sym', bias=True, **kwargs):\n",
    "        super(TGCNConv4, self).__init__(in_channels, out_channels, K, normalization, bias, **kwargs)\n",
    "        self.lin = nn.Linear(in_channels, out_channels)\n",
    "        # Initialize weights for the ChebConv layers\n",
    "        nn.init.kaiming_uniform_(self.lin.weight, mode='fan_in', nonlinearity='relu')\n",
    "        nn.init.zeros_(self.lin.bias)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        # L2 normalization on input features\n",
    "        x = F.normalize(x, p=2, dim=1)\n",
    "        edge_attr = edge_attr.view(-1, 1)\n",
    "        return super(TGCNConv4, self).forward(x, edge_index, edge_attr)\n",
    "        \n",
    "class no_PFs_Model2(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim_ae, hidden_dim_rgcn, output_dim, num_relations, num_bases, num_time_embeddings, dropout_rate=0.5):\n",
    "        super(no_PFs_Model2, self).__init__()\n",
    "        self.hidden_dim_ae = hidden_dim_ae\n",
    "        self.hidden_dim_rgcn = hidden_dim_rgcn\n",
    "        self.autoencoder = Autoencoder4(input_dim, hidden_dim_ae)\n",
    "        self.time_embedding = nn.Embedding(num_time_embeddings, hidden_dim_ae)\n",
    "        self.rgcn1 = TGCNConv4(hidden_dim_ae + hidden_dim_ae, hidden_dim_rgcn, K=2, normalization='sym')\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)\n",
    "        self.rgcn2 = TGCNConv4(hidden_dim_rgcn, hidden_dim_rgcn, K=2, normalization='sym')\n",
    "        self.dropout2 = nn.Dropout(dropout_rate)\n",
    "        self.linear = nn.Linear(hidden_dim_rgcn, output_dim)\n",
    "        self.linear2 = nn.Linear(output_dim, output_dim)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_attr = data.x, data.edge_index, data.edge_attr\n",
    "        \n",
    "        encoded, _ = self.autoencoder(x)\n",
    "        time_embedding = self.time_embedding(edge_attr.long())\n",
    "\n",
    "        if time_embedding.dim() == 3:\n",
    "            time_embedding = time_embedding.squeeze(1)\n",
    "\n",
    "        x_with_temporal = torch.cat([encoded, time_embedding], dim=1)\n",
    "        x_with_temporal = x_with_temporal.view(-1, self.hidden_dim_ae * 2)\n",
    "\n",
    "        x = F.relu(self.rgcn1(x_with_temporal, edge_index, edge_attr))\n",
    "        x = self.dropout1(x)\n",
    "        x = F.relu(self.rgcn2(x, edge_index, edge_attr))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.linear(x)\n",
    "        x = self.linear2(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Initialize the proposed method\n",
    "input_dim_ae = train_data.x.shape[1]\n",
    "num_time_embeddings = 10  # Example value\n",
    "num_relations = df['relation_label'].nunique()  # Example value\n",
    "output_dim = 1  # Binary classification (trust/distrust)\n",
    "\n",
    "no_PFsConf_model = no_PFs_Model2(\n",
    "    input_dim_ae,\n",
    "    hidden_dim_ae,\n",
    "    hidden_dim_rgcn,\n",
    "    output_dim,\n",
    "    num_relations,\n",
    "    num_bases=2,\n",
    "    num_time_embeddings=num_time_embeddings\n",
    ")\n",
    "\n",
    "# Define loss function and optimizer with weight decay\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(no_PFsConf_model.parameters(), lr, weight_decay=1e-4)\n",
    "\n",
    "start_time = time.time()\n",
    "# Train the model with dropout and weight decay\n",
    "no_PFsConf_model.train()\n",
    "for epoch in range(N_Epo):\n",
    "    optimizer.zero_grad()\n",
    "    out = no_PFsConf_model(train_data)\n",
    "\n",
    "    target = train_data.y.float().view(-1, 1)\n",
    "\n",
    "    loss = criterion(out, target)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch {epoch}, Loss: {loss.item()}')\n",
    "\n",
    "end_time = time.time()\n",
    "# Evaluation on test data\n",
    "no_PFsConf_model.eval()\n",
    "with torch.no_grad():\n",
    "    no_PFsConf_model_pred = no_PFsConf_model(test_data)\n",
    "\n",
    "\n",
    "\n",
    "no_PFsConf_pred_probabilities = torch.sigmoid(no_PFsConf_model_pred)\n",
    "\n",
    "\n",
    "runtime = end_time - start_time\n",
    "print(\"Runtime:\", runtime, \"seconds\")\n",
    "no_PFsConf_time = runtime\n",
    "\n",
    "\n",
    "no_PFsConf_pred_labels = ((no_PFsConf_model_pred) > torch.mean(no_PFsConf_model_pred)).int()\n",
    "# Compute AUC\n",
    "no_PFsConf_auc = roc_auc_score(test_labels.numpy(), no_PFsConf_pred_labels.numpy())\n",
    "\n",
    "\n",
    "# Get the predicted rankings for each edge\n",
    "indices, _ = torch.sort(no_PFsConf_model_pred, descending=True)\n",
    "ranks = torch.zeros_like(indices, dtype=torch.float)  # Initialize ranks\n",
    "\n",
    "# Loop through the sorted indices to calculate ranks\n",
    "for i, idx in enumerate(indices):\n",
    "    ranks[i] = idx + 1  # Adjust rank starting from 1\n",
    "\n",
    "# Number of unlabeled links\n",
    "num_positive_links = torch.sum(test_labels == 1)\n",
    "num_negative_links = torch.sum(test_labels == 0)\n",
    "denominator = num_positive_links / (num_positive_links - num_negative_links)\n",
    "\n",
    "# Calculate RS for each positive link\n",
    "positive_indices = test_labels == 1\n",
    "positive_ranks = ranks[positive_indices]\n",
    "RSe_values = positive_ranks / denominator\n",
    "no_PFsConf_RS = torch.mean(RSe_values).item()\n",
    "\n",
    "print('Evaluation results for proposed without polynomial features and confidence:')\n",
    "print(f'AUC: {no_PFsConf_auc}, RS: {no_PFsConf_RS}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de0d7e2-e5bf-4b4a-94ad-8e9c44c5192a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3221313e-594a-42f6-9159-5f530b351285",
   "metadata": {},
   "outputs": [],
   "source": [
    "# actual values\n",
    "actual = test_labels.numpy()\n",
    "# predicted values\n",
    "predicted = (no_PFsConf_pred_probabilities >= torch.mean(no_PFsConf_pred_probabilities)).int()\n",
    "\n",
    "# confusion matrix\n",
    "no_PFsConf_matrix = confusion_matrix(actual,predicted, labels=[1,0])\n",
    "print('Confusion matrix : \\n',no_PFsConf_matrix)\n",
    "\n",
    "# outcome values order in sklearn\n",
    "tp, fn, fp, tn = confusion_matrix(actual,predicted,labels=[1,0]).reshape(-1)\n",
    "print('Outcome values : \\n', tp, fn, fp, tn)\n",
    "\n",
    "# classification report for precision, recall f1-score and accuracy\n",
    "no_PFsConf_matrix = classification_report(actual,predicted,labels=[1,0])\n",
    "print('Classification report : \\n',no_PFsConf_matrix)\n",
    "\n",
    "# Calculate Accuracy, Precision, Recall, F1\n",
    "no_PFsConf_accuracy = accuracy_score(test_labels.numpy(), predicted.numpy())\n",
    "no_PFsConf_precision = precision_score(test_labels.numpy(), predicted.numpy())\n",
    "no_PFsConf_recall = recall_score(test_labels.numpy(), predicted.numpy())\n",
    "no_PFsConf_f1 = f1_score(test_labels.numpy(), predicted.numpy())\n",
    "print(f'Accuracy: {no_PFsConf_accuracy}, Precision: {no_PFsConf_precision}, Recall: {no_PFsConf_recall}, F1: {no_PFsConf_f1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ec3263-def8-482c-9926-9635810e1043",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d46310-a5b3-432d-925d-292e53d98f90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5869df31-debe-4eba-8cb9-a9f805a3b89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### saving the results\n",
    "\n",
    "results = {\n",
    "    'Metric': ['AUC', 'RS', 'Accuracy', 'Precision', 'Recall', 'F1'],\n",
    "    'Without PFs and Confidence': [no_PFsConf_auc, no_PFsConf_RS, no_PFsConf_accuracy, no_PFsConf_precision, no_PFsConf_recall, no_PFsConf_f1],\n",
    "    'Without Confidence': [no_confidence_auc, no_confidence_RS, no_confidence_accuracy, no_confidence_precision, no_confidence_recall, no_confidence_f1],\n",
    "    'Without PFs': [no_PFs_auc, no_PFs_RS, no_PFs_accuracy, no_PFs_precision, no_PFs_recall, no_PFs_f1],\n",
    "    'TGCN': [ourmodel_auc, ourmodel_RS, ourmodel_accuracy, ourmodel_precision, ourmodel_recall, ourmodel_f1]\n",
    "}\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14936ed-c9e0-461f-9256-1ab6e9ff6743",
   "metadata": {},
   "outputs": [],
   "source": [
    "### saving the results\n",
    "# Create a DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Save to Excel\n",
    "results_df.to_excel('results/withConfusionMatrix/results_200epoch.xlsx', index=False)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15904391-ce93-4f13-a93e-0896d498d2d8",
   "metadata": {},
   "source": [
    "### Plotting the results of proposed approach and its variant on bar chart for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5dd9503-77ed-4127-a084-6925d7a8c3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming `results_df` is your DataFrame containing the results\n",
    "rows_to_drop = [\"Accuracy\", \"Precision\", \"Recall\", \"F1\"]\n",
    "\n",
    "# Filter rows where the \"Metric\" column is not in rows_to_drop\n",
    "metrics_filter1 = results_df[~results_df['Metric'].isin(rows_to_drop)]\n",
    "\n",
    "# Metrics and labels\n",
    "metrics = metrics_filter1['Metric']\n",
    "no_PFsConf_values = metrics_filter1['Without PFs and Confidence']\n",
    "no_Conf_values = metrics_filter1['Without Confidence']\n",
    "no_PFs_values = metrics_filter1['Without PFs']\n",
    "TGCN_values = metrics_filter1['TGCN']\n",
    "\n",
    "# Bar width and positions\n",
    "bar_width = 0.15\n",
    "index = range(len(metrics))\n",
    "positions1 = [i - 1.5 * bar_width for i in index]\n",
    "positions2 = [i - 0.5 * bar_width for i in index]\n",
    "positions3 = [i + 0.5 * bar_width for i in index]\n",
    "positions4 = [i + 1.5 * bar_width for i in index]\n",
    "\n",
    "# Colors\n",
    "colors = ['blue', 'green', 'orange', 'purple']\n",
    "\n",
    "# Create bar chart\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "bar1 = ax.bar(positions1, no_PFsConf_values, bar_width, label='Model 1', color=colors[0])\n",
    "bar2 = ax.bar(positions2, no_Conf_values, bar_width, label='Model 2', color=colors[1])\n",
    "bar3 = ax.bar(positions3, no_PFs_values, bar_width, label='Model 3', color=colors[2])\n",
    "bar4 = ax.bar(positions4, TGCN_values, bar_width, label='R-CTGCN', color=colors[3])\n",
    "\n",
    "# Add labels on top of each bar\n",
    "for bars, values in zip([bar1, bar2, bar3, bar4], [no_PFsConf_values, no_Conf_values, no_PFs_values, TGCN_values]):\n",
    "    for bar, value in zip(bars, values):\n",
    "        yval = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2, yval + 0.005, round(value, 3), ha='center', va='bottom')\n",
    "\n",
    "# Adjust other chart elements\n",
    "ax.set_xlabel('Metrics')\n",
    "ax.set_ylabel('Values')\n",
    "ax.set_title('Comparison of Different Model Configurations')\n",
    "ax.set_xticks([i for i in index])\n",
    "ax.set_xticklabels(metrics)\n",
    "ax.legend(loc='upper center', bbox_to_anchor=(0.5, 1.15), ncol=4)\n",
    "\n",
    "# Save the figure\n",
    "plt.savefig('results/withConfusionMatrix/chart_Epinion_200epoch.png', bbox_inches='tight')\n",
    "\n",
    "# Display the chart\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d29cbf7-9373-4c0f-872e-88551f31f1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "rows_to_drop = [\"AUC\", \"RS\"]\n",
    "# Filter rows where the \"Metric\" column is not in rows_to_drop\n",
    "metrics_filter2 = results_df[~results_df['Metric'].isin(rows_to_drop)]\n",
    "\n",
    "# Metrics and labels\n",
    "metrics = metrics_filter2['Metric']\n",
    "no_PFsConf_values = metrics_filter2['Without PFs and Confidence']\n",
    "no_Conf_values = metrics_filter2['Without Confidence']\n",
    "no_PFs_values = metrics_filter2['Without PFs']\n",
    "TGCN_values = metrics_filter2['TGCN']\n",
    "\n",
    "# Bar width and positions\n",
    "bar_width = 0.2\n",
    "index = range(len(metrics))\n",
    "positions1 = [i - 1.5 * bar_width for i in index]\n",
    "positions2 = [i - 0.5 * bar_width for i in index]\n",
    "positions3 = [i + 0.5 * bar_width for i in index]\n",
    "positions4 = [i + 1.5 * bar_width for i in index]\n",
    "\n",
    "# Colors\n",
    "colors = ['blue', 'green', 'orange', 'purple']\n",
    "\n",
    "# Create bar chart\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "bar1 = ax.bar(positions1, no_PFsConf_values, bar_width, label='Model 1', color=colors[0])\n",
    "bar2 = ax.bar(positions2, no_Conf_values, bar_width, label='Model 2', color=colors[1])\n",
    "bar3 = ax.bar(positions3, no_PFs_values, bar_width, label='Model 3', color=colors[2])\n",
    "bar4 = ax.bar(positions4, TGCN_values, bar_width, label='R-CTGCN', color=colors[3])\n",
    "\n",
    "# Add labels on top of each bar\n",
    "for bars, values in zip([bar1, bar2, bar3, bar4], [no_PFsConf_values, no_Conf_values, no_PFs_values, TGCN_values]):\n",
    "    for bar, value in zip(bars, values):\n",
    "        yval = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2, yval + 0.005, round(value, 3), ha='center', va='bottom')\n",
    "\n",
    "# Adjust other chart elements\n",
    "ax.set_xlabel('Metrics')\n",
    "ax.set_ylabel('Values')\n",
    "ax.set_title('Comparison of Different Model Configurations')\n",
    "ax.set_xticks([i for i in index])\n",
    "ax.set_xticklabels(metrics, rotation=45, ha='right')\n",
    "ax.legend(loc='upper center', bbox_to_anchor=(0.5, 1.15), ncol=4)\n",
    "\n",
    "\n",
    "# Save the figure\n",
    "plt.savefig('results/withConfusionMatrix/Confusionmatrix_chart_Epinion_200epoch.png', bbox_inches='tight')\n",
    "\n",
    "# Display the chart\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d097c22-a139-4df7-817c-873814a5e318",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152f31e1-4445-4776-a210-014371e12ff9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "69e292cc-f3a9-480e-a203-95cad5ebb9b3",
   "metadata": {},
   "source": [
    "# Comparison with Mirror and RGCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c89384e-e1ec-492e-9685-b43821ed41a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a random weight matrix with values between 0 and 1\n",
    "random_weight_matrix = np.random.rand(df.shape[0], 1)\n",
    "df['jaccard_similarity'] = df['jaccard_similarity'] / df['jaccard_similarity'].max()\n",
    "df['adamic_similarity'] = df['adamic_similarity'] / df['adamic_similarity'].max()\n",
    "# Combine matrices using horizontal stacking\n",
    "aggregated_matrix = np.hstack([df['jaccard_similarity'].values.reshape(-1, 1), \n",
    "                               df['adamic_similarity'].values.reshape(-1, 1), \n",
    "                               random_weight_matrix])\n",
    "\n",
    "df['aggregated_similarity'] = aggregated_matrix[:, 2]\n",
    "#df['aggregated_similarity'] = df['aggregated_similarity'] * 0.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e6391c-86f0-4c34-b52b-ad99d328b5d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9b5d92fb-2f66-4fb2-bba7-082e15b0a0f7",
   "metadata": {},
   "source": [
    "### Mirror"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e5bc75-98a4-4894-9c48-bde993670643",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Split the dataset into training and testing sets\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "train_features = torch.tensor(train_df[['jaccard_similarity', 'aggregated_similarity']].values, dtype=torch.float)\n",
    "test_features = torch.tensor(test_df[['jaccard_similarity', 'aggregated_similarity']].values, dtype=torch.float)\n",
    "edge_index_train_np = np.array([train_df['source_index'].values, train_df['target_index'].values])\n",
    "edge_index_test_np = np.array([test_df['source_index'].values, test_df['target_index'].values])\n",
    "edge_index_train = torch.tensor(edge_index_train_np, dtype=torch.long)\n",
    "edge_index_test = torch.tensor(edge_index_test_np, dtype=torch.long)\n",
    "train_labels = torch.tensor(train_df['relation_label'].values, dtype=torch.long)\n",
    "test_labels = torch.tensor(test_df['relation_label'].values, dtype=torch.long)\n",
    "train_data = Data(x=train_features, edge_index=edge_index_train, y=train_labels, edge_type=train_labels)\n",
    "test_data = Data(x=test_features, edge_index=edge_index_test, y=test_labels, edge_type=test_labels)\n",
    "from torch_geometric.nn import RGCNConv\n",
    "# Define the autoencoder model\n",
    "class MirrorAutoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(MirrorAutoencoder, self).__init__()\n",
    "        self.encoder = nn.Linear(input_dim, hidden_dim)\n",
    "        self.decoder = nn.Linear(hidden_dim, input_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = F.relu(self.encoder(x))\n",
    "        decoded = torch.sigmoid(self.decoder(encoded))\n",
    "        return encoded, decoded\n",
    "\n",
    "# Define the TrustRGCNAutoencoder model with the autoencoder\n",
    "class Mirror(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim_ae, hidden_dim_rgcn, output_dim, num_relations, num_bases):\n",
    "        super(Mirror, self).__init__()\n",
    "        self.autoencoder = MirrorAutoencoder(input_dim, hidden_dim_ae)\n",
    "        self.rgcn1 = ChebConv(hidden_dim_ae, hidden_dim_rgcn, K=2, normalization='sym')\n",
    "        self.rgcn2 = RGCNConv(hidden_dim_rgcn, hidden_dim_rgcn, num_relations, num_bases=2)\n",
    "        self.rgcn3 = ChebConv(hidden_dim_rgcn, hidden_dim_rgcn, K=2, normalization='sym')\n",
    "        self.rgcn4 = RGCNConv(hidden_dim_rgcn, output_dim, num_relations, num_bases=2)\n",
    "\n",
    "        \n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_type = data.x, data.edge_index, data.edge_type\n",
    "        encoded, _ = self.autoencoder(x)\n",
    "        x = F.relu(self.rgcn1(encoded, edge_index))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = F.relu(self.rgcn2(x, edge_index, edge_type))\n",
    "        x = F.relu(self.rgcn3(x, edge_index))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.rgcn4(x, edge_index, edge_type)\n",
    "        return torch.sigmoid(x)\n",
    "\n",
    "\n",
    "# Initialize the Mirror method\n",
    "input_dim_ae = train_data.x.shape[1]\n",
    "num_relations = df['relation_label'].nunique()  # Example value\n",
    "output_dim = 1  # Binary classification\n",
    "\n",
    "Mirror_model = Mirror(input_dim_ae, hidden_dim_ae, hidden_dim_rgcn, output_dim, num_relations, num_bases=2)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.BCELoss() # BCELoss  # BCEWithLogitsLoss\n",
    "optimizer = torch.optim.Adam(Mirror_model.parameters(), lr)\n",
    "# .optim.SGD\n",
    "# Train the model\n",
    "Mirror_model.train()\n",
    "for epoch in range(N_Epo):\n",
    "    optimizer.zero_grad()\n",
    "    out = Mirror_model(train_data)\n",
    "    \n",
    "    # Modify target to match the output shape\n",
    "    target = train_data.y.float().view(-1, 1)\n",
    "    \n",
    "    loss = criterion(out, target)\n",
    "    #loss2 = loss * 0.1\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Print training loss for monitoring\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch {epoch}, Loss: {loss.item()}')\n",
    "\n",
    "\n",
    "# Evaluation on test data\n",
    "Mirror_model.eval()\n",
    "with torch.no_grad():\n",
    "    pred = Mirror_model(test_data)\n",
    "\n",
    "\n",
    "# Apply sigmoid to get probability scores\n",
    "Mirrorpred_probabilities = torch.sigmoid(pred)\n",
    "\n",
    "\n",
    "# Compute AUC\n",
    "Mirror_auc = roc_auc_score(test_labels.numpy(), Mirrorpred_probabilities.numpy())\n",
    "\n",
    "\n",
    "# Get the predicted rankings for each edge\n",
    "indices, _ = torch.sort(Mirrorpred_probabilities, descending=True)\n",
    "ranks = torch.zeros_like(indices, dtype=torch.float)  # Initialize ranks\n",
    "\n",
    "# Loop through the sorted indices to calculate ranks\n",
    "for i, idx in enumerate(indices):\n",
    "    ranks[i] = idx + 1  # Adjust rank starting from 1\n",
    "\n",
    "# Number of unlabeled links\n",
    "\n",
    "num_positive_links = torch.sum(test_labels == 1)\n",
    "num_negative_links = torch.sum(test_labels == 0)\n",
    "denominator = (num_positive_links / (num_positive_links - num_negative_links))\n",
    "\n",
    "# Calculate RS for each positive link\n",
    "positive_indices = test_labels == 1\n",
    "positive_ranks = ranks[positive_indices]\n",
    "RSe_values = positive_ranks / denominator\n",
    "\n",
    "Mirror_RS = torch.mean(RSe_values).item()\n",
    "\n",
    "print('Evaluation results for Mirror:')\n",
    "print(f'AUC: {Mirror_auc}, RS: {Mirror_RS}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00c545a-6388-40a8-8a9d-d041cbdac3a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed8b224-8a3f-4ac8-92de-f9d0591aee10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# actual values\n",
    "actual = test_labels.numpy()\n",
    "# predicted values\n",
    "predicted = (Mirrorpred_probabilities >= torch.mean(pred)).int()\n",
    "\n",
    "\n",
    "# confusion matrix\n",
    "Mirror_matrix = confusion_matrix(actual,predicted, labels=[1,0])\n",
    "print('Confusion matrix : \\n',Mirror_matrix)\n",
    "\n",
    "# outcome values order in sklearn\n",
    "tp, fn, fp, tn = confusion_matrix(actual,predicted,labels=[1,0]).reshape(-1)\n",
    "print('Outcome values : \\n', tp, fn, fp, tn)\n",
    "\n",
    "# classification report for precision, recall f1-score and accuracy\n",
    "Mirror_matrix = classification_report(actual,predicted,labels=[1,0])\n",
    "print('Classification report : \\n',Mirror_matrix)\n",
    "\n",
    "# Calculate Accuracy, Precision, Recall, F1\n",
    "Mirror_accuracy = accuracy_score(test_labels.numpy(), predicted.numpy())\n",
    "Mirror_precision = precision_score(test_labels.numpy(), predicted.numpy())\n",
    "Mirror_recall = recall_score(test_labels.numpy(), predicted.numpy())\n",
    "Mirror_f1 = f1_score(test_labels.numpy(), predicted.numpy())\n",
    "print(f'Accuracy: {Mirror_accuracy}, Precision: {Mirror_precision}, Recall: {Mirror_recall}, F1: {Mirror_f1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb4afd7-a1de-4067-990d-df5f4dd4f1b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "51488eed-c8e5-4929-b3f3-39ceeb79ef93",
   "metadata": {},
   "source": [
    "### RGCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb578955-4b6e-4185-b4ef-6718fee8dd0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = torch.tensor(train_df[['jaccard_similarity', 'adamic_similarity']].values, dtype=torch.float)\n",
    "test_features = torch.tensor(test_df[['jaccard_similarity', 'adamic_similarity']].values, dtype=torch.float)\n",
    "edge_index_train_np = np.array([train_df['source_index'].values, train_df['target_index'].values])\n",
    "edge_index_test_np = np.array([test_df['source_index'].values, test_df['target_index'].values])\n",
    "edge_index_train = torch.tensor(edge_index_train_np, dtype=torch.long)\n",
    "edge_index_test = torch.tensor(edge_index_test_np, dtype=torch.long)\n",
    "train_labels = torch.tensor(train_df['relation_label'].values, dtype=torch.long)\n",
    "test_labels = torch.tensor(test_df['relation_label'].values, dtype=torch.long)\n",
    "train_data = Data(x=train_features, edge_index=edge_index_train, y=train_labels, edge_type=train_labels)\n",
    "test_data = Data(x=test_features, edge_index=edge_index_test, y=test_labels, edge_type=test_labels)\n",
    "\n",
    "from torch_geometric.nn import RGCNConv\n",
    "\n",
    "# Define the TrustRGCN model with the autoencoder\n",
    "class RGCN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim_rgcn, output_dim, num_relations):\n",
    "        super(RGCN, self).__init__()\n",
    "        self.rgcn1 = RGCNConv(input_dim, hidden_dim_rgcn, num_relations, num_bases=2)\n",
    "        self.rgcn2 = RGCNConv(hidden_dim_rgcn, hidden_dim_rgcn, num_relations, num_bases=2)\n",
    "        self.rgcn3 = RGCNConv(hidden_dim_rgcn, output_dim, num_relations, num_bases=2)\n",
    "        \n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_type = data.x, data.edge_index, data.edge_type\n",
    "        x = F.normalize(x, p=2, dim=1)\n",
    "        x = F.relu(self.rgcn1(x, edge_index, edge_type))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = F.relu(self.rgcn2(x, edge_index, edge_type))\n",
    "        x = self.rgcn3(x, edge_index, edge_type)\n",
    "        return torch.sigmoid(x)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Initialize the TrustRGCNAutoencoder model\n",
    "input_dim_ae = train_data.x.shape[1]\n",
    "num_relations = df['relation_label'].nunique()  # Example value\n",
    "output_dim = 1  # Binary classification (trust/distrust)\n",
    "\n",
    "RGCNmodel = RGCN(input_dim_ae, hidden_dim_rgcn, output_dim, num_relations)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adadelta(RGCNmodel.parameters(), lr)\n",
    "\n",
    "\n",
    "# Train the model\n",
    "RGCNmodel.train()\n",
    "for epoch in range(N_Epo):  # You can adjust the number of epochs\n",
    "    optimizer.zero_grad()\n",
    "    out = RGCNmodel(train_data)\n",
    "    target = train_data.y.float().view(-1, 1)\n",
    "    loss = criterion(out, target)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Print training loss for monitoring\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch {epoch}, Loss: {loss.item()}')\n",
    "\n",
    "\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "RGCNmodel.eval()\n",
    "with torch.no_grad():\n",
    "    pred_rgcn = RGCNmodel(test_data)\n",
    "\n",
    "# Apply sigmoid to get probability scores\n",
    "pred_rgcn_probabilities = torch.sigmoid(pred_rgcn)\n",
    "\n",
    "# Compute AUC\n",
    "RGCN_auc = roc_auc_score(test_labels.numpy(), (pred_rgcn_probabilities >= torch.mean(pred_rgcn_probabilities)).int().numpy())\n",
    "\n",
    "\n",
    "# Get the predicted rankings for each edge\n",
    "indices, _ = torch.sort(pred_rgcn_probabilities, descending=True)\n",
    "ranks = torch.zeros_like(indices, dtype=torch.float)  # Initialize ranks\n",
    "\n",
    "# Loop through the sorted indices to calculate ranks\n",
    "for i, idx in enumerate(indices):\n",
    "    ranks[i] = idx + 1  # Adjust rank starting from 1\n",
    "\n",
    "# Number of unlabeled links\n",
    "num_positive_links = torch.sum(test_labels == 1)\n",
    "num_negative_links = torch.sum(test_labels == 0)\n",
    "denominator = (num_positive_links / (num_positive_links - num_negative_links))\n",
    "\n",
    "# Calculate RS for each positive link\n",
    "positive_indices = test_labels == 1\n",
    "positive_ranks = ranks[positive_indices]\n",
    "RSe_values = positive_ranks / denominator\n",
    "RGCN_RS = torch.mean(RSe_values).item()\n",
    "\n",
    "print('Evaluation results for R-GCN:')\n",
    "print(f'AUC: {RGCN_auc}, RS: {RGCN_RS}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e85b87-3248-4d51-9ead-45b60aa879dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36c7abf-9e98-4e18-9c4c-3ce06dd88839",
   "metadata": {},
   "outputs": [],
   "source": [
    "# actual values\n",
    "actual = test_labels.numpy()\n",
    "# predicted values\n",
    "predicted = (pred_rgcn_probabilities >= torch.mean(pred_rgcn_probabilities)).int()\n",
    "\n",
    "\n",
    "# confusion matrix\n",
    "RGCN_matrix = confusion_matrix(actual,predicted, labels=[1,0])\n",
    "print('Confusion matrix : \\n',RGCN_matrix)\n",
    "\n",
    "# outcome values order in sklearn\n",
    "tp, fn, fp, tn = confusion_matrix(actual,predicted,labels=[1,0]).reshape(-1)\n",
    "print('Outcome values : \\n', tp, fn, fp, tn)\n",
    "\n",
    "# classification report for precision, recall f1-score and accuracy\n",
    "RGCN_matrix = classification_report(actual,predicted,labels=[1,0])\n",
    "print('Classification report : \\n',RGCN_matrix)\n",
    "\n",
    "# Calculate Accuracy, Precision, Recall, F1\n",
    "RGCN_accuracy = accuracy_score(test_labels.numpy(), predicted.numpy())\n",
    "RGCN_precision = precision_score(test_labels.numpy(), predicted.numpy())\n",
    "RGCN_recall = recall_score(test_labels.numpy(), predicted.numpy())\n",
    "RGCN_f1 = f1_score(test_labels.numpy(), predicted.numpy())\n",
    "print(f'Accuracy: {RGCN_accuracy}, Precision: {RGCN_precision}, Recall: {RGCN_recall}, F1: {RGCN_f1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95952937-198f-4034-9aff-66fe29bf8067",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed20efb-7a3c-4050-8fc7-c1561c6b7d20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cf8d4f9c-1e17-4456-a9e4-60f7b857f3e0",
   "metadata": {},
   "source": [
    "### Add resutls of Mirror and RGCN to the results df and resave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51ff7f3-4413-4363-8cd2-65da04cda9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df['Mirror'] = [Mirror_auc, Mirror_RS, Mirror_accuracy, Mirror_precision, Mirror_recall, Mirror_f1]\n",
    "results_df['R-GCN'] = [RGCN_auc, RGCN_RS, RGCN_accuracy, RGCN_precision, RGCN_recall, RGCN_f1]\n",
    "\n",
    "# Save new one to Excel\n",
    "results_df.to_excel('results/withConfusionMatrix/results_200epoch.xlsx', index=False)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d727c85-0985-413e-b35c-fb9f40c47d0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9002e8d0-fe04-41fa-a5d6-21483b333c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_to_drop = [\"Accuracy\", \"Precision\", \"Recall\", \"F1\"]\n",
    "\n",
    "# Filter rows where the \"Metric\" column is not in rows_to_drop\n",
    "metrics_filter1 = results_df[~results_df['Metric'].isin(rows_to_drop)]\n",
    "metrics_filter1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12928c23-4d59-4db5-bfbe-2ed5ce363d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_to_drop = [\"AUC\", \"RS\"]\n",
    "# Filter rows where the \"Metric\" column is not in rows_to_drop\n",
    "metrics_filter2 = results_df[~results_df['Metric'].isin(rows_to_drop)]\n",
    "metrics_filter2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fbe231f-27d4-4502-9a6c-fcc16d4ed6c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6ba5fa-6f6b-4ff1-9772-46f936969e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Metrics and labels\n",
    "metrics = metrics_filter1['Metric']\n",
    "RCTGCN_values = metrics_filter1['TGCN']\n",
    "RGCN_values = metrics_filter1['R-GCN']\n",
    "Mirror_values = metrics_filter1['Mirror']\n",
    "\n",
    "# Bar width and positions\n",
    "bar_width = 0.2\n",
    "index = range(len(metrics))\n",
    "positions1 = [i - 1.1 * bar_width for i in index]\n",
    "positions2 = [i for i in index]\n",
    "positions3 = [i + 1.1 * bar_width for i in index]\n",
    "\n",
    "# Colors\n",
    "colors = ['blue', 'green', 'orange', 'purple', 'maroon', 'cyan']\n",
    "\n",
    "# Create bar chart\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "bar1 = ax.bar(positions1, RCTGCN_values, bar_width, label='R-CTGCN', color=colors[3])\n",
    "bar2 = ax.bar(positions2, RGCN_values, bar_width, label='R-GCN', color=colors[4])\n",
    "bar3 = ax.bar(positions3, Mirror_values, bar_width, label='Mirror', color=colors[5])\n",
    "\n",
    "# Add labels on top of each bar\n",
    "all_bars = [bar1, bar2, bar3]\n",
    "all_values = [RCTGCN_values, RGCN_values, Mirror_values]\n",
    "for bars, values in zip(all_bars, all_values):\n",
    "    for bar, value in zip(bars, values):\n",
    "        yval = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2, yval + 0.005, round(value, 3), ha='center', va='bottom')\n",
    "\n",
    "# Adjust other chart elements\n",
    "ax.set_xlabel('Metrics')\n",
    "ax.set_ylabel('Values')\n",
    "ax.set_title('Comparison of Different Models')\n",
    "ax.set_xticks([i for i in index])\n",
    "ax.set_xticklabels(metrics)\n",
    "ax.legend(loc='upper center', bbox_to_anchor=(0.5, 1.15), ncol=6)\n",
    "\n",
    "# Save the new figure\n",
    "plt.savefig('results/withConfusionMatrix/02_chart_Epinion_200epoch_2.png', bbox_inches='tight')\n",
    "\n",
    "# Display the chart\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c094ea66-5267-4316-ab95-144598182d29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d62e2d9-9c51-4d0a-b39b-e74bb951a017",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Metrics and labels\n",
    "metrics = metrics_filter2['Metric']\n",
    "RCTGCN_values = metrics_filter2['TGCN']\n",
    "RGCN_values = metrics_filter2['R-GCN']\n",
    "Mirror_values = metrics_filter2['Mirror']\n",
    "\n",
    "# Bar width and positions\n",
    "bar_width = 0.2\n",
    "index = range(len(metrics))\n",
    "positions1 = [i - 1.1 * bar_width for i in index]\n",
    "positions2 = [i for i in index]\n",
    "positions3 = [i + 1.1 * bar_width for i in index]\n",
    "\n",
    "# Colors\n",
    "colors = ['purple', 'maroon', 'cyan']\n",
    "\n",
    "# Create bar chart\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "bar1 = ax.bar(positions1, RCTGCN_values, bar_width, label='R-CTGCN', color=colors[0])\n",
    "bar2 = ax.bar(positions2, RGCN_values, bar_width, label='R-GCN', color=colors[1])\n",
    "bar3 = ax.bar(positions3, Mirror_values, bar_width, label='Mirror', color=colors[2])\n",
    "\n",
    "# Add labels on top of each bar\n",
    "all_bars = [bar1, bar2, bar3]\n",
    "all_values = [RCTGCN_values, RGCN_values, Mirror_values]\n",
    "for bars, values in zip(all_bars, all_values):\n",
    "    for bar, value in zip(bars, values):\n",
    "        yval = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2, yval + 0.005, round(value, 3), ha='center', va='bottom')\n",
    "\n",
    "# Adjust other chart elements\n",
    "ax.set_xlabel('Metrics')\n",
    "ax.set_ylabel('Values')\n",
    "ax.set_title('Comparison of Different Models')\n",
    "ax.set_xticks([i for i in index])\n",
    "ax.set_xticklabels(metrics)\n",
    "ax.legend(loc='upper center', bbox_to_anchor=(0.5, 1.15), ncol=6)\n",
    "\n",
    "# Save the new figure\n",
    "plt.savefig('results/withConfusionMatrix/02_Confusionmatrix_chart_Epinion_200epoch_2.png', bbox_inches='tight')\n",
    "\n",
    "# Display the chart\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a3650c-e56a-4eb2-bdeb-3ce4dd61a579",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26fad6e4-6d01-4179-823e-405003892ad7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
