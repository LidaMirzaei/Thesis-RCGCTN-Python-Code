{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d633c57b-1e89-4921-8063-73fc1404f269",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  # NumPy for numerical operations\n",
    "import pandas as pd  # Pandas for data manipulation and analysis\n",
    "from scipy.io import loadmat\n",
    "# Importing PyTorch and PyG modules, and the necessary libraries for this end\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import ChebConv\n",
    "from torch_geometric.nn import RGCNConv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, f1_score, precision_score, recall_score\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5410884-ba8a-4828-859e-4f57d9a50536",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "enron_path = 'Useable data/ia-enron-email-dynamic/enron_dynamic.csv'\n",
    "\n",
    "\n",
    "# Read the trust dataset from the specified file\n",
    "df = pd.read_csv(enron_path)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d73617-8f31-4378-84ea-f4b313da9e4b",
   "metadata": {},
   "source": [
    "# Proposed Confidence metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8759a6a1-89a6-4717-bc9a-0c86d6dc6422",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the combined confidence score\n",
    "numerator = 2 * df['jaccard_similarity'] * df['adamic_similarity']   # * df['common_neighbor']\n",
    "denominator = df['jaccard_similarity'] + df['adamic_similarity']     # + df['common_neighbor']\n",
    "\n",
    "# Handle potential division by zero\n",
    "df.loc[:, 'confidence'] = np.where(denominator != 0, (numerator / denominator), 0)\n",
    "\n",
    "# Normalize the confidence scores to the range [0, 1]\n",
    "df.loc[:, 'confidence'] = df['confidence'] / df['confidence'].max()\n",
    "\n",
    "# Print the DataFrame with the new columns\n",
    "df[['source_id', 'target_id', 'jaccard_similarity', 'adamic_similarity', 'confidence']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ed2c87-2d9b-4b85-8b4d-b5226aebc91c",
   "metadata": {},
   "source": [
    "# Preparing data for test and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e6ccfb-e4f8-4f09-977d-b89819d7bbad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepares data for a PyTorch Geometric model\n",
    "\n",
    "# Assuming you have unique node IDs for source users and target users\n",
    "source_users = df['source_id'].unique()\n",
    "target_users = df['target_id'].unique()\n",
    "\n",
    "# Create a mapping of node IDs to indices\n",
    "node_to_index = {node: index for index, node in enumerate(set(source_users) | set(target_users))}\n",
    "\n",
    "# Map node IDs in the dataframe to indices\n",
    "df.loc[:,'source_index'] = df['source_id'].map(node_to_index)\n",
    "df.loc[:,'target_index'] = df['target_id'].map(node_to_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8f5b78-54ea-488c-8dfc-b8118737ff4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming df has columns 'source_id', 'target_id', 'timestamp', and others\n",
    "# Convert timestamps to Unix time\n",
    "df['timesdate'] = pd.to_datetime(df['timestamp'])\n",
    "df['timesdate'] = df['timesdate'].astype('int64') // 1e9\n",
    "df = df.sort_values(by='timestamp')\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81614346-bab8-4e9a-be67-8bb7bc71445e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'adamic_similarity',  'jaccard_similarity', \n",
    "# Concatenate polynomial features with the original features\n",
    "train_features = torch.tensor(train_df[['jaccard_similarity', 'confidence']].values, dtype=torch.float)\n",
    "test_features = torch.tensor(test_df[['jaccard_similarity', 'confidence']].values, dtype=torch.float)\n",
    "\n",
    "\n",
    "# Convert list of NumPy arrays to a single NumPy array\n",
    "edge_index_train_np = np.array([train_df['source_index'].values, train_df['target_index'].values])\n",
    "edge_index_test_np = np.array([test_df['source_index'].values, test_df['target_index'].values])\n",
    "# Convert the NumPy arrays to PyTorch tensors\n",
    "edge_index_train = torch.tensor(edge_index_train_np, dtype=torch.long)\n",
    "edge_index_test = torch.tensor(edge_index_test_np, dtype=torch.long)\n",
    "\n",
    "# 'timestamp', 'normalized_time'\n",
    "edge_attr_train = torch.tensor(train_df['normalized_time'].values, dtype=torch.float).view(-1, 1)\n",
    "edge_attr_test = torch.tensor(test_df['normalized_time'].values, dtype=torch.float).view(-1, 1)\n",
    "\n",
    "train_labels = torch.tensor(train_df['relation_label'].values, dtype=torch.long)\n",
    "test_labels = torch.tensor(test_df['relation_label'].values, dtype=torch.long)\n",
    "\n",
    "train_data = Data(x=train_features, edge_index=edge_index_train, edge_attr=edge_attr_train, y=train_labels)\n",
    "test_data = Data(x=test_features, edge_index=edge_index_test, edge_attr=edge_attr_test, y=test_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be63ae5c-3876-470e-87cb-7934bb67c798",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.001\n",
    "N_Epo = 200\n",
    "hidden_dim_ae = 64\n",
    "hidden_dim_rgcn = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02cee897-10fa-475b-8516-e24301abbf4b",
   "metadata": {},
   "source": [
    "# Proposed Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93f0a09-f63e-4675-952c-9c6184d794d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the autoencoder model\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, activation_fn=nn.ReLU()):\n",
    "        super(Autoencoder, self).__init__()\n",
    "\n",
    "        # Encoder layers\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            activation_fn\n",
    "        )\n",
    "\n",
    "        # Decoder layers\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, input_dim),\n",
    "            nn.Sigmoid()  # Sigmoid activation for reconstruction\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return encoded, decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa9047c-42f6-409c-8dde-e25e960adcb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TGCNConv(ChebConv):\n",
    "    def __init__(self, in_channels, out_channels, K=2, normalization='sym', bias=True, **kwargs):\n",
    "        super(TGCNConv, self).__init__(in_channels, out_channels, K, normalization, bias, **kwargs)\n",
    "        self.lin = nn.Linear(in_channels, out_channels)\n",
    "        # Initialize weights for the ChebConv layers\n",
    "        nn.init.kaiming_uniform_(self.lin.weight, mode='fan_in', nonlinearity='relu')\n",
    "        nn.init.zeros_(self.lin.bias)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        # L2 normalization on input features\n",
    "        x = F.normalize(x, p=2, dim=1)\n",
    "        edge_attr = edge_attr.view(-1, 1)\n",
    "        return super(TGCNConv, self).forward(x, edge_index, edge_attr)\n",
    "\n",
    "class RealationPrediction_Model(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim_ae, hidden_dim_rgcn, output_dim, num_relations, num_bases, num_time_embeddings, poly_degree, dropout_rate=0.5):\n",
    "        super(RealationPrediction_Model, self).__init__()\n",
    "        self.hidden_dim_ae = hidden_dim_ae\n",
    "        self.hidden_dim_rgcn = hidden_dim_rgcn\n",
    "        self.autoencoder = Autoencoder(input_dim * poly_degree, hidden_dim_ae)\n",
    "        self.time_embedding = nn.Embedding(num_time_embeddings, hidden_dim_ae)\n",
    "        \n",
    "        self.rgcn = TGCNConv(hidden_dim_ae, hidden_dim_rgcn, K=2, normalization='sym')\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)\n",
    "        self.rgcn1 = TGCNConv(hidden_dim_rgcn, hidden_dim_rgcn, K=2, normalization='sym')\n",
    "        self.dropout2 = nn.Dropout(dropout_rate)\n",
    "\n",
    "        self.rgcn2 = RGCNConv(hidden_dim_rgcn, hidden_dim_rgcn, num_relations, num_bases=2)\n",
    "        self.linear = nn.Linear(hidden_dim_rgcn, output_dim)\n",
    "\n",
    "        self.poly_degree = poly_degree\n",
    "        \n",
    "        #nn.init.kaiming_uniform_(self.linear.weight, mode='fan_in', nonlinearity='relu')\n",
    "        #nn.init.zeros_(self.linear.bias)\n",
    "\n",
    "    \n",
    "    def generate_polynomial_features(self, x):\n",
    "        poly_features = [x[:, i] ** d for d in range(1, self.poly_degree + 1) for i in range(x.shape[1])]\n",
    "        return torch.stack(poly_features, dim=1)\n",
    "\n",
    "    \n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_attr, edge_type = data.x, data.edge_index, data.edge_attr, data.y\n",
    "        \n",
    "        poly_features = self.generate_polynomial_features(x)\n",
    "        encoded, _ = self.autoencoder(poly_features)\n",
    "        time_embedding = self.time_embedding(edge_attr.long())\n",
    "        if time_embedding.dim() == 3:\n",
    "            time_embedding = time_embedding.squeeze(1)\n",
    "            \n",
    "        x_with_temporal = torch.cat([encoded, time_embedding], dim=1)\n",
    "        x_with_temporal = x_with_temporal.view(-1, self.hidden_dim_ae * 2)\n",
    "\n",
    "        x = F.relu(self.rgcn(encoded, edge_index, edge_attr))\n",
    "        x = self.dropout1(x)\n",
    "        x = F.relu(self.rgcn1(x, edge_index, edge_attr))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.rgcn2(x, edge_index, edge_type)\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Initialize the proposed method\n",
    "input_dim_ae = train_data.x.shape[1]\n",
    "num_time_embeddings = 10  # Example value\n",
    "num_relations = df['relation_label'].nunique()  # Example value\n",
    "output_dim = 1  # Binary classification (trust/distrust)\n",
    "\n",
    "proposed_model = RealationPrediction_Model(\n",
    "    input_dim_ae,\n",
    "    hidden_dim_ae,\n",
    "    hidden_dim_rgcn,\n",
    "    output_dim,\n",
    "    num_relations,\n",
    "    num_bases=2,\n",
    "    num_time_embeddings=num_time_embeddings,\n",
    "    poly_degree=2\n",
    ")\n",
    "\n",
    "# Define loss function and optimizer with weight decay\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(proposed_model.parameters(), lr, weight_decay=1e-4)\n",
    "\n",
    "start_time = time.time()\n",
    "# Train the model with dropout and weight decay\n",
    "proposed_model.train()\n",
    "for epoch in range(N_Epo):\n",
    "    optimizer.zero_grad()\n",
    "    out = proposed_model(train_data)\n",
    "\n",
    "    target = train_data.y.float().view(-1, 1)\n",
    "\n",
    "    loss = criterion(out, target)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch {epoch}, Loss: {loss.item()}')\n",
    "\n",
    "end_time = time.time()\n",
    "# Evaluation on test data\n",
    "proposed_model.eval()\n",
    "with torch.no_grad():\n",
    "    proposed_model_pred = proposed_model(test_data)\n",
    "\n",
    "\n",
    "\n",
    "runtime = end_time - start_time\n",
    "print(\"Runtime:\", runtime, \"seconds\")\n",
    "ourmodel_time = runtime\n",
    "\n",
    "proposed_model_probabilities = torch.sigmoid(proposed_model_pred)\n",
    "\n",
    "# Compute AUC\n",
    "ourmodel_auc = roc_auc_score(test_labels.numpy(), proposed_model_probabilities.numpy())\n",
    "\n",
    "\n",
    "# Get the predicted rankings for each edge\n",
    "indices, _ = torch.sort(proposed_model_probabilities, descending=True)\n",
    "ranks = torch.zeros_like(indices, dtype=torch.float)  # Initialize ranks\n",
    "\n",
    "# Loop through the sorted indices to calculate ranks\n",
    "for i, idx in enumerate(indices):\n",
    "    ranks[i] = idx + 1  # Adjust rank starting from 1\n",
    "\n",
    "# Number of unlabeled links\n",
    "num_positive_links = torch.sum(test_labels == 1)\n",
    "num_negative_links = torch.sum(test_labels == 0)\n",
    "denominator = num_positive_links / (num_positive_links - num_negative_links)\n",
    "\n",
    "# Calculate RS for each positive link\n",
    "positive_indices = test_labels == 1\n",
    "positive_ranks = ranks[positive_indices]\n",
    "RSe_values = positive_ranks / denominator\n",
    "ourmodel_RS = torch.mean(RSe_values).item()\n",
    "\n",
    "\n",
    "print(f'AUC: {ourmodel_auc}, RS: {ourmodel_RS}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aceca518-1b3e-4b51-986c-c7ab68e4bc88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538c33f9-5cb2-4c4a-902e-bee933631bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrix in sklearn\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# actual values\n",
    "actual = test_labels.numpy()\n",
    "# predicted values\n",
    "predicted = (proposed_model_probabilities >= torch.mean(proposed_model_probabilities)).int()\n",
    "\n",
    "\n",
    "# confusion matrix\n",
    "ourmodel_matrix = confusion_matrix(actual,predicted, labels=[1,0])\n",
    "print('Confusion matrix : \\n',ourmodel_matrix)\n",
    "\n",
    "# outcome values order in sklearn\n",
    "tp, fn, fp, tn = confusion_matrix(actual,predicted,labels=[1,0]).reshape(-1)\n",
    "print('Outcome values : \\n', tp, fn, fp, tn)\n",
    "\n",
    "# classification report for precision, recall f1-score and accuracy\n",
    "ourmodel_matrix = classification_report(actual,predicted,labels=[1,0])\n",
    "print('Classification report : \\n',ourmodel_matrix)\n",
    "\n",
    "# Calculate Accuracy, Precision, Recall, F1\n",
    "ourmodel_accuracy = accuracy_score(test_labels.numpy(), predicted.numpy())\n",
    "ourmodel_precision = precision_score(test_labels.numpy(), predicted.numpy())\n",
    "ourmodel_recall = recall_score(test_labels.numpy(), predicted.numpy())\n",
    "ourmodel_f1 = f1_score(test_labels.numpy(), predicted.numpy())\n",
    "print(f'Accuracy: {ourmodel_accuracy}, Precision: {ourmodel_precision}, Recall: {ourmodel_recall}, F1: {ourmodel_f1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4e4dff-9828-4cee-88ef-d7cb4674d035",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "432d3169-0514-4350-bf38-8b4c13a2c48e",
   "metadata": {},
   "source": [
    "# Compared Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d084a1cd-7f90-42f3-a194-e6d5957ec1d1",
   "metadata": {},
   "source": [
    "## Other variant of our proposed "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58046de0-1d91-4d71-8325-5143e71521e0",
   "metadata": {},
   "source": [
    "### With out PFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf03c74-93e4-48d0-beb0-f519e4444a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = torch.tensor(train_df[['jaccard_similarity', 'adamic_similarity', 'confidence']].values, dtype=torch.float)\n",
    "test_features = torch.tensor(test_df[['jaccard_similarity', 'adamic_similarity', 'confidence']].values, dtype=torch.float)\n",
    "edge_index_train_np = np.array([train_df['source_index'].values, train_df['target_index'].values])\n",
    "edge_index_test_np = np.array([test_df['source_index'].values, test_df['target_index'].values])\n",
    "edge_index_train = torch.tensor(edge_index_train_np, dtype=torch.long)\n",
    "edge_index_test = torch.tensor(edge_index_test_np, dtype=torch.long)\n",
    "edge_attr_train = torch.tensor(train_df['normalized_time'].values, dtype=torch.float).view(-1, 1)\n",
    "edge_attr_test = torch.tensor(test_df['normalized_time'].values, dtype=torch.float).view(-1, 1)\n",
    "train_labels = torch.tensor(train_df['relation_label'].values, dtype=torch.long)\n",
    "test_labels = torch.tensor(test_df['relation_label'].values, dtype=torch.long)\n",
    "train_data = Data(x=train_features, edge_index=edge_index_train, edge_attr=edge_attr_train, y=train_labels)\n",
    "test_data = Data(x=test_features, edge_index=edge_index_test, edge_attr=edge_attr_test, y=test_labels)\n",
    "\n",
    "# Define the autoencoder model\n",
    "class Autoencoder2(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, activation_fn=nn.ReLU()):\n",
    "        super(Autoencoder2, self).__init__()\n",
    "\n",
    "        # Encoder layers\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            activation_fn\n",
    "        )\n",
    "\n",
    "        # Decoder layers\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, input_dim),\n",
    "            nn.Sigmoid()  # Sigmoid activation for reconstruction\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return encoded, decoded\n",
    "        \n",
    "class TGCNConv2(ChebConv):\n",
    "    def __init__(self, in_channels, out_channels, K=2, normalization='sym', bias=True, **kwargs):\n",
    "        super(TGCNConv2, self).__init__(in_channels, out_channels, K, normalization, bias, **kwargs)\n",
    "        self.lin = nn.Linear(in_channels, out_channels)\n",
    "        # Initialize weights for the ChebConv layers\n",
    "        nn.init.kaiming_uniform_(self.lin.weight, mode='fan_in', nonlinearity='relu')\n",
    "        nn.init.zeros_(self.lin.bias)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        # L2 normalization on input features\n",
    "        x = F.normalize(x, p=2, dim=1)\n",
    "        edge_attr = edge_attr.view(-1, 1)\n",
    "        return super(TGCNConv2, self).forward(x, edge_index, edge_attr)\n",
    "        \n",
    "class no_PFs_Model(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim_ae, hidden_dim_rgcn, output_dim, num_relations, num_bases, num_time_embeddings, dropout_rate=0.5):\n",
    "        super(no_PFs_Model, self).__init__()\n",
    "        self.hidden_dim_ae = hidden_dim_ae\n",
    "        self.hidden_dim_rgcn = hidden_dim_rgcn\n",
    "        self.autoencoder = Autoencoder2(input_dim, hidden_dim_ae)\n",
    "        self.time_embedding = nn.Embedding(num_time_embeddings, hidden_dim_ae)\n",
    "        self.rgcn1 = TGCNConv2(hidden_dim_ae + hidden_dim_ae, hidden_dim_rgcn, K=2, normalization='sym')\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)\n",
    "        self.rgcn2 = TGCNConv2(hidden_dim_rgcn, hidden_dim_rgcn, K=2, normalization='sym')\n",
    "        self.dropout2 = nn.Dropout(dropout_rate)\n",
    "        self.linear = nn.Linear(hidden_dim_rgcn, hidden_dim_rgcn)\n",
    "        self.linear2 = nn.Linear(hidden_dim_rgcn, output_dim)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_attr = data.x, data.edge_index, data.edge_attr\n",
    "        \n",
    "        encoded, _ = self.autoencoder(x)\n",
    "        time_embedding = self.time_embedding(edge_attr.long())\n",
    "\n",
    "        if time_embedding.dim() == 3:\n",
    "            time_embedding = time_embedding.squeeze(1)\n",
    "\n",
    "        x_with_temporal = torch.cat([encoded, time_embedding], dim=1)\n",
    "        x_with_temporal = x_with_temporal.view(-1, self.hidden_dim_ae * 2)\n",
    "\n",
    "        x = F.relu(self.rgcn1(x_with_temporal, edge_index, edge_attr))\n",
    "        x = self.dropout1(x)\n",
    "        x = F.relu(self.rgcn2(x, edge_index, edge_attr))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.linear(x)\n",
    "        x = torch.sigmoid(self.linear2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "# Initialize the proposed method\n",
    "input_dim_ae = train_data.x.shape[1]\n",
    "num_time_embeddings = 10  # Example value\n",
    "num_relations = df['relation_label'].nunique()  # Example value\n",
    "output_dim = 1  # Binary classification (trust/distrust)\n",
    "\n",
    "no_PFs_model = no_PFs_Model(\n",
    "    input_dim_ae,\n",
    "    hidden_dim_ae,\n",
    "    hidden_dim_rgcn,\n",
    "    output_dim,\n",
    "    num_relations,\n",
    "    num_bases=2,\n",
    "    num_time_embeddings=num_time_embeddings\n",
    ")\n",
    "\n",
    "# Define loss function and optimizer with weight decay\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(no_PFs_model.parameters(), lr, weight_decay=1e-4)\n",
    "\n",
    "start_time = time.time()\n",
    "# Train the model with dropout and weight decay\n",
    "no_PFs_model.train()\n",
    "for epoch in range(N_Epo):\n",
    "    optimizer.zero_grad()\n",
    "    out = no_PFs_model(train_data)\n",
    "\n",
    "    target = train_data.y.float().view(-1, 1)\n",
    "\n",
    "    loss = criterion(out, target)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch {epoch}, Loss: {loss.item()}')\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "# Evaluation on test data\n",
    "no_PFs_model.eval()\n",
    "with torch.no_grad():\n",
    "    no_PFs_model_pred = no_PFs_model(test_data)\n",
    "\n",
    "\n",
    "\n",
    "no_PFs_pred_probabilities = torch.sigmoid(no_PFs_model_pred)\n",
    "\n",
    "\n",
    "runtime = end_time - start_time\n",
    "print(\"Runtime:\", runtime, \"seconds\")\n",
    "no_PFs_time = runtime\n",
    "\n",
    "\n",
    "# Compute AUC\n",
    "# Compute AUC\n",
    "no_PFs_auc = roc_auc_score(test_labels.numpy(), no_PFs_pred_probabilities.numpy())\n",
    "\n",
    "# Get the predicted rankings for each edge\n",
    "indices, _ = torch.sort(no_PFs_model_pred, descending=True)\n",
    "ranks = torch.zeros_like(indices, dtype=torch.float)  # Initialize ranks\n",
    "\n",
    "# Loop through the sorted indices to calculate ranks\n",
    "for i, idx in enumerate(indices):\n",
    "    ranks[i] = idx + 1  # Adjust rank starting from 1\n",
    "\n",
    "# Number of unlabeled links\n",
    "num_positive_links = torch.sum(test_labels == 1)\n",
    "num_negative_links = torch.sum(test_labels == 0)\n",
    "denominator = num_positive_links / (num_positive_links - num_negative_links)\n",
    "\n",
    "# Calculate RS for each positive link\n",
    "positive_indices = test_labels == 1\n",
    "positive_ranks = ranks[positive_indices]\n",
    "RSe_values = positive_ranks / denominator\n",
    "no_PFs_RS = torch.mean(RSe_values).item()\n",
    "\n",
    "print('Evaluation results for proposed without polynomial features:')\n",
    "print(f'AUC: {no_PFs_auc}, RS: {no_PFs_RS}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4a7a01-2da2-4c4a-a15c-fb393b3aebfc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c67f581-5700-49b7-a75c-e7f0cc8a118a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# actual values\n",
    "actual = test_labels.numpy()\n",
    "# predicted values\n",
    "predicted = (no_PFs_pred_probabilities > torch.mean(no_PFs_pred_probabilities)).int()\n",
    "\n",
    "\n",
    "# confusion matrix\n",
    "no_PFs_matrix = confusion_matrix(actual,predicted, labels=[1,0])\n",
    "print('Confusion matrix : \\n',no_PFs_matrix)\n",
    "\n",
    "# outcome values order in sklearn\n",
    "tp, fn, fp, tn = confusion_matrix(actual,predicted,labels=[1,0]).reshape(-1)\n",
    "print('Outcome values : \\n', tp, fn, fp, tn)\n",
    "\n",
    "# classification report for precision, recall f1-score and accuracy\n",
    "no_PFs_matrix = classification_report(actual,predicted,labels=[1,0])\n",
    "print('Classification report : \\n',no_PFs_matrix)\n",
    "\n",
    "# Calculate Accuracy, Precision, Recall, F1\n",
    "no_PFs_accuracy = accuracy_score(test_labels.numpy(), predicted.numpy())\n",
    "no_PFs_precision = precision_score(test_labels.numpy(), predicted.numpy())\n",
    "no_PFs_recall = recall_score(test_labels.numpy(), predicted.numpy())\n",
    "no_PFs_f1 = f1_score(test_labels.numpy(), predicted.numpy())\n",
    "print(f'Accuracy: {no_PFs_accuracy}, Precision: {no_PFs_precision}, Recall: {no_PFs_recall}, F1: {no_PFs_f1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d46d71-d817-42df-b5f8-f9d5abd74d8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a4b0cddf-c18e-4aca-8607-298fbe30bca7",
   "metadata": {},
   "source": [
    "### With out confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03aade57-e4fb-4f0a-8b2d-84b8fd6db821",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = torch.tensor(train_df[['jaccard_similarity','adamic_similarity']].values, dtype=torch.float)\n",
    "test_features = torch.tensor(test_df[['jaccard_similarity','adamic_similarity']].values, dtype=torch.float)\n",
    "edge_index_train_np = np.array([train_df['source_index'].values, train_df['target_index'].values])\n",
    "edge_index_test_np = np.array([test_df['source_index'].values, test_df['target_index'].values])\n",
    "edge_index_train = torch.tensor(edge_index_train_np, dtype=torch.long)\n",
    "edge_index_test = torch.tensor(edge_index_test_np, dtype=torch.long)\n",
    "edge_attr_train = torch.tensor(train_df['normalized_time'].values, dtype=torch.float).view(-1, 1)\n",
    "edge_attr_test = torch.tensor(test_df['normalized_time'].values, dtype=torch.float).view(-1, 1)\n",
    "train_labels = torch.tensor(train_df['relation_label'].values, dtype=torch.long)\n",
    "test_labels = torch.tensor(test_df['relation_label'].values, dtype=torch.long)\n",
    "train_data = Data(x=train_features, edge_index=edge_index_train, edge_attr=edge_attr_train, y=train_labels)\n",
    "test_data = Data(x=test_features, edge_index=edge_index_test, edge_attr=edge_attr_test, y=test_labels)\n",
    "# Define the autoencoder model\n",
    "class Autoencoder3(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, activation_fn=nn.ReLU()):\n",
    "        super(Autoencoder3, self).__init__()\n",
    "\n",
    "        # Encoder layers\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            activation_fn\n",
    "        )\n",
    "\n",
    "        # Decoder layers\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, input_dim),\n",
    "            nn.Sigmoid()  # Sigmoid activation for reconstruction\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return encoded, decoded\n",
    "        \n",
    "class TGCNConv3(ChebConv):\n",
    "    def __init__(self, in_channels, out_channels, K=2, normalization='sym', bias=True, **kwargs):\n",
    "        super(TGCNConv3, self).__init__(in_channels, out_channels, K, normalization, bias, **kwargs)\n",
    "        self.lin = nn.Linear(in_channels, out_channels)\n",
    "        # Initialize weights for the ChebConv layers\n",
    "        nn.init.kaiming_uniform_(self.lin.weight, mode='fan_in', nonlinearity='relu')\n",
    "        nn.init.zeros_(self.lin.bias)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        # L2 normalization on input features\n",
    "        x = F.normalize(x, p=2, dim=1)\n",
    "        edge_attr = edge_attr.view(-1, 1)\n",
    "        return super(TGCNConv3, self).forward(x, edge_index, edge_attr)\n",
    "\n",
    "\n",
    "class RealationPrediction_Model_2(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim_ae, hidden_dim_rgcn, output_dim, num_relations, num_bases, num_time_embeddings, poly_degree, dropout_rate=0.5):\n",
    "        super(RealationPrediction_Model_2, self).__init__()\n",
    "        self.hidden_dim_ae = hidden_dim_ae\n",
    "        self.hidden_dim_rgcn = hidden_dim_rgcn\n",
    "        self.autoencoder = Autoencoder3(input_dim * poly_degree, hidden_dim_ae)\n",
    "        self.time_embedding = nn.Embedding(num_time_embeddings, hidden_dim_ae)\n",
    "        self.rgcn1 = TGCNConv3(hidden_dim_ae + hidden_dim_ae, hidden_dim_rgcn, K=2, normalization='sym')\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)\n",
    "        self.rgcn2 = TGCNConv3(hidden_dim_rgcn, hidden_dim_rgcn, K=2, normalization='sym')\n",
    "        self.dropout2 = nn.Dropout(dropout_rate)\n",
    "        self.linear = nn.Linear(hidden_dim_rgcn, hidden_dim_rgcn)\n",
    "        self.linear2 = nn.Linear(hidden_dim_rgcn, output_dim)\n",
    "        self.poly_degree = poly_degree\n",
    "        \n",
    "        # Initialize weights for the linear layer\n",
    "        nn.init.kaiming_uniform_(self.linear.weight, mode='fan_in', nonlinearity='relu')\n",
    "        nn.init.zeros_(self.linear.bias)\n",
    "\n",
    "    def generate_polynomial_features(self, x):\n",
    "        poly_features = [x[:, i] ** d for d in range(1, self.poly_degree + 1) for i in range(x.shape[1])]\n",
    "        return torch.stack(poly_features, dim=1)\n",
    "\n",
    "    \n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_attr = data.x, data.edge_index, data.edge_attr\n",
    "        \n",
    "        poly_features = self.generate_polynomial_features(x)\n",
    "        encoded, _ = self.autoencoder(poly_features)\n",
    "        time_embedding = self.time_embedding(edge_attr.long())\n",
    "\n",
    "        if time_embedding.dim() == 3:\n",
    "            time_embedding = time_embedding.squeeze(1)\n",
    "\n",
    "        x_with_temporal = torch.cat([encoded, time_embedding], dim=1)\n",
    "        x_with_temporal = x_with_temporal.view(-1, self.hidden_dim_ae * 2)\n",
    "\n",
    "        x = F.relu(self.rgcn1(x_with_temporal, edge_index, edge_attr))\n",
    "        x = self.dropout1(x)\n",
    "        x = F.relu(self.rgcn2(x, edge_index, edge_attr))\n",
    "        x = self.dropout2(x)\n",
    "        x = F.relu(self.rgcn2(x, edge_index, edge_attr))\n",
    "        x = self.dropout2(x)\n",
    "        x = torch.sigmoid(self.linear2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "# Initialize the proposed method\n",
    "input_dim_ae = train_data.x.shape[1]\n",
    "num_time_embeddings = 10  # Example value\n",
    "num_relations = df['relation_label'].nunique()  # Example value\n",
    "output_dim = 1  # Binary classification (trust/distrust)\n",
    "\n",
    "no_confidence_model = RealationPrediction_Model_2(\n",
    "    input_dim_ae,\n",
    "    hidden_dim_ae,\n",
    "    hidden_dim_rgcn,\n",
    "    output_dim,\n",
    "    num_relations,\n",
    "    num_bases=2,\n",
    "    num_time_embeddings=num_time_embeddings,\n",
    "    poly_degree=2\n",
    ")\n",
    "\n",
    "# Define loss function and optimizer with weight decay\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(no_confidence_model.parameters(), lr, weight_decay=1e-4)\n",
    "\n",
    "start_time = time.time()\n",
    "# Train the model with dropout and weight decay\n",
    "no_confidence_model.train()\n",
    "for epoch in range(N_Epo):\n",
    "    optimizer.zero_grad()\n",
    "    out = no_confidence_model(train_data)\n",
    "\n",
    "    target = train_data.y.float().view(-1, 1)\n",
    "\n",
    "    loss = criterion(out, target)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch {epoch}, Loss: {loss.item()}')\n",
    "\n",
    "end_time = time.time()\n",
    "# Evaluation on test data\n",
    "no_confidence_model.eval()\n",
    "with torch.no_grad():\n",
    "    no_confidence_model_pred = no_confidence_model(test_data)\n",
    "\n",
    "\n",
    "no_confidence_probabilities =  torch.sigmoid(no_confidence_model_pred)\n",
    "\n",
    "runtime = end_time - start_time\n",
    "print(\"Runtime:\", runtime, \"seconds\")\n",
    "no_confidence_time = runtime\n",
    "\n",
    "# Compute AUC\n",
    "no_confidence_auc = roc_auc_score(test_labels.numpy(), no_confidence_probabilities.numpy())\n",
    "\n",
    "# Get the predicted rankings for each edge\n",
    "indices, _ = torch.sort(no_confidence_probabilities, descending=True)\n",
    "ranks = torch.zeros_like(indices, dtype=torch.float)  # Initialize ranks\n",
    "\n",
    "# Loop through the sorted indices to calculate ranks\n",
    "for i, idx in enumerate(indices):\n",
    "    ranks[i] = idx + 1  # Adjust rank starting from 1\n",
    "\n",
    "# Number of unlabeled links\n",
    "num_positive_links = torch.sum(test_labels == 1)\n",
    "num_negative_links = torch.sum(test_labels == 0)\n",
    "denominator = num_positive_links / (num_positive_links - num_negative_links)\n",
    "\n",
    "# Calculate RS for each positive link\n",
    "positive_indices = test_labels == 1\n",
    "positive_ranks = ranks[positive_indices]\n",
    "RSe_values = positive_ranks / denominator\n",
    "no_confidence_RS = torch.mean(RSe_values).item()\n",
    "\n",
    "print('Evaluation results for proposed without confidence:')\n",
    "print(f'AUC: {no_confidence_auc}, RS: {no_confidence_RS}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5752e8-8e23-4635-b1d2-d39cb6f9c4f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8227264a-f645-4735-8859-51056991a4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# actual values\n",
    "actual = test_labels.numpy()\n",
    "# predicted values\n",
    "predicted = (no_confidence_probabilities >= torch.mean(no_confidence_probabilities)).int()\n",
    "\n",
    "\n",
    "# confusion matrix\n",
    "no_confidence_matrix = confusion_matrix(actual,predicted, labels=[1,0])\n",
    "print('Confusion matrix : \\n',no_confidence_matrix)\n",
    "\n",
    "# outcome values order in sklearn\n",
    "tp, fn, fp, tn = confusion_matrix(actual,predicted,labels=[1,0]).reshape(-1)\n",
    "print('Outcome values : \\n', tp, fn, fp, tn)\n",
    "\n",
    "# classification report for precision, recall f1-score and accuracy\n",
    "no_confidence_matrix = classification_report(actual,predicted,labels=[1,0])\n",
    "print('Classification report : \\n',no_confidence_matrix)\n",
    "\n",
    "# Calculate Accuracy, Precision, Recall, F1\n",
    "no_confidence_accuracy = accuracy_score(test_labels.numpy(), predicted.numpy())\n",
    "no_confidence_precision = precision_score(test_labels.numpy(), predicted.numpy())\n",
    "no_confidence_recall = recall_score(test_labels.numpy(), predicted.numpy())\n",
    "no_confidence_f1 = f1_score(test_labels.numpy(), predicted.numpy())\n",
    "print(f'Accuracy: {no_confidence_accuracy}, Precision: {no_confidence_precision}, Recall: {no_confidence_recall}, F1: {no_confidence_f1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757e4e4a-04b9-4b3e-bf3b-b3f673a56287",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "45a7bf23-ffe5-4483-b951-1cef6ae8f6f6",
   "metadata": {},
   "source": [
    "### With out both PFs and Confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4394773-e9e3-4756-b50f-b29cbaa2888d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = torch.tensor(train_df[['jaccard_similarity','adamic_similarity']].values, dtype=torch.float)\n",
    "test_features = torch.tensor(test_df[['jaccard_similarity','adamic_similarity']].values, dtype=torch.float)\n",
    "edge_index_train_np = np.array([train_df['source_index'].values, train_df['target_index'].values])\n",
    "edge_index_test_np = np.array([test_df['source_index'].values, test_df['target_index'].values])\n",
    "edge_index_train = torch.tensor(edge_index_train_np, dtype=torch.long)\n",
    "edge_index_test = torch.tensor(edge_index_test_np, dtype=torch.long)\n",
    "edge_attr_train = torch.tensor(train_df['normalized_time'].values, dtype=torch.float).view(-1, 1)\n",
    "edge_attr_test = torch.tensor(test_df['normalized_time'].values, dtype=torch.float).view(-1, 1)\n",
    "train_labels = torch.tensor(train_df['relation_label'].values, dtype=torch.long)\n",
    "test_labels = torch.tensor(test_df['relation_label'].values, dtype=torch.long)\n",
    "train_data = Data(x=train_features, edge_index=edge_index_train, edge_attr=edge_attr_train, y=train_labels)\n",
    "test_data = Data(x=test_features, edge_index=edge_index_test, edge_attr=edge_attr_test, y=test_labels)\n",
    "\n",
    "# Define the autoencoder model\n",
    "class Autoencoder4(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, activation_fn=nn.ReLU()):\n",
    "        super(Autoencoder4, self).__init__()\n",
    "\n",
    "        # Encoder layers\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            activation_fn\n",
    "        )\n",
    "\n",
    "        # Decoder layers\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, input_dim),\n",
    "            nn.Sigmoid()  # Sigmoid activation for reconstruction\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return encoded, decoded\n",
    "        \n",
    "class TGCNConv4(ChebConv):\n",
    "    def __init__(self, in_channels, out_channels, K=2, normalization='sym', bias=True, **kwargs):\n",
    "        super(TGCNConv4, self).__init__(in_channels, out_channels, K, normalization, bias, **kwargs)\n",
    "        self.lin = nn.Linear(in_channels, out_channels)\n",
    "        # Initialize weights for the ChebConv layers\n",
    "        nn.init.kaiming_uniform_(self.lin.weight, mode='fan_in', nonlinearity='relu')\n",
    "        nn.init.zeros_(self.lin.bias)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        # L2 normalization on input features\n",
    "        x = F.normalize(x, p=1, dim=1)\n",
    "        edge_attr = edge_attr.view(-1, 1)\n",
    "        return super(TGCNConv4, self).forward(x, edge_index, edge_attr)\n",
    "        \n",
    "class no_PFs_Model2(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim_ae, hidden_dim_rgcn, output_dim, num_relations, num_bases, num_time_embeddings, dropout_rate=0.5):\n",
    "        super(no_PFs_Model2, self).__init__()\n",
    "        self.hidden_dim_ae = hidden_dim_ae\n",
    "        self.hidden_dim_rgcn = hidden_dim_rgcn\n",
    "        self.autoencoder = Autoencoder4(input_dim, hidden_dim_ae)\n",
    "        self.time_embedding = nn.Embedding(num_time_embeddings, hidden_dim_ae)\n",
    "        self.rgcn1 = TGCNConv4(hidden_dim_ae + hidden_dim_ae, hidden_dim_rgcn, K=2, normalization='sym')\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)\n",
    "        self.rgcn2 = TGCNConv4(hidden_dim_rgcn, hidden_dim_rgcn, K=2, normalization='sym')\n",
    "        self.dropout2 = nn.Dropout(dropout_rate)\n",
    "        self.linear = nn.Linear(hidden_dim_rgcn, hidden_dim_rgcn)\n",
    "        self.linear2 = nn.Linear(hidden_dim_rgcn, output_dim)\n",
    " \n",
    "        # Initialize weights for the linear layer\n",
    "        nn.init.kaiming_uniform_(self.linear.weight, mode='fan_in', nonlinearity='relu')\n",
    "        nn.init.zeros_(self.linear.bias)\n",
    "\n",
    "    \n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_attr = data.x, data.edge_index, data.edge_attr\n",
    "        \n",
    "        encoded, _ = self.autoencoder(x)\n",
    "        time_embedding = self.time_embedding(edge_attr.long())\n",
    "\n",
    "        if time_embedding.dim() == 3:\n",
    "            time_embedding = time_embedding.squeeze(1)\n",
    "\n",
    "        x_with_temporal = torch.cat([encoded, time_embedding], dim=1)\n",
    "        x_with_temporal = x_with_temporal.view(-1, self.hidden_dim_ae * 2)\n",
    "\n",
    "        x = F.relu(self.rgcn1(x_with_temporal, edge_index, edge_attr))\n",
    "        x = self.dropout1(x)\n",
    "        x = F.relu(self.rgcn2(x, edge_index, edge_attr))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.linear(x)\n",
    "        x = self.linear2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Initialize the proposed method\n",
    "input_dim_ae = train_data.x.shape[1]\n",
    "num_time_embeddings = 10  # Example value\n",
    "num_relations = df['relation_label'].nunique()  # Example value\n",
    "output_dim = 1  # Binary classification (trust/distrust)\n",
    "\n",
    "no_PFsConf_model = no_PFs_Model2(\n",
    "    input_dim_ae,\n",
    "    hidden_dim_ae,\n",
    "    hidden_dim_rgcn,\n",
    "    output_dim,\n",
    "    num_relations,\n",
    "    num_bases=2,\n",
    "    num_time_embeddings=num_time_embeddings\n",
    ")\n",
    "\n",
    "# Define loss function and optimizer with weight decay\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(no_PFsConf_model.parameters(), lr, weight_decay=1e-4)\n",
    "\n",
    "start_time = time.time()\n",
    "# Train the model with dropout and weight decay\n",
    "no_PFsConf_model.train()\n",
    "for epoch in range(N_Epo):\n",
    "    optimizer.zero_grad()\n",
    "    out = no_PFsConf_model(train_data)\n",
    "\n",
    "    target = train_data.y.float().view(-1, 1)\n",
    "\n",
    "    loss = criterion(out, target)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch {epoch}, Loss: {loss.item()}')\n",
    "\n",
    "end_time = time.time()\n",
    "# Evaluation on test data\n",
    "no_PFsConf_model.eval()\n",
    "with torch.no_grad():\n",
    "    no_PFsConf_model_pred = no_PFsConf_model(test_data)\n",
    "\n",
    "\n",
    "\n",
    "no_PFsConf_pred_probabilities = torch.sigmoid(no_PFsConf_model_pred)\n",
    "\n",
    "\n",
    "runtime = end_time - start_time\n",
    "print(\"Runtime:\", runtime, \"seconds\")\n",
    "no_PFsConf_time = runtime\n",
    "\n",
    "\n",
    "no_PFsConf_pred_labels = ((no_PFsConf_model_pred) > torch.mean(no_PFsConf_model_pred)).int()\n",
    "# Compute AUC\n",
    "no_PFsConf_auc = roc_auc_score(test_labels.numpy(), no_PFsConf_pred_labels.numpy())\n",
    "\n",
    "\n",
    "# Get the predicted rankings for each edge\n",
    "indices, _ = torch.sort(no_PFsConf_model_pred, descending=True)\n",
    "ranks = torch.zeros_like(indices, dtype=torch.float)  # Initialize ranks\n",
    "\n",
    "# Loop through the sorted indices to calculate ranks\n",
    "for i, idx in enumerate(indices):\n",
    "    ranks[i] = idx + 1  # Adjust rank starting from 1\n",
    "\n",
    "# Number of unlabeled links\n",
    "num_positive_links = torch.sum(test_labels == 1)\n",
    "num_negative_links = torch.sum(test_labels == 0)\n",
    "denominator = num_positive_links / (num_positive_links - num_negative_links)\n",
    "\n",
    "# Calculate RS for each positive link\n",
    "positive_indices = test_labels == 1\n",
    "positive_ranks = ranks[positive_indices]\n",
    "RSe_values = positive_ranks / denominator\n",
    "no_PFsConf_RS = torch.mean(RSe_values).item()\n",
    "\n",
    "print('Evaluation results for proposed without polynomial features and confidence:')\n",
    "print(f'AUC: {no_PFsConf_auc}, RS: {no_PFsConf_RS}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e54184-b420-4a51-8b60-be82b0eaa5fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178b479f-6866-4eb9-bf0d-13391406b3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# actual values\n",
    "actual = test_labels.numpy()\n",
    "# predicted values\n",
    "predicted = (no_PFsConf_pred_probabilities >= torch.mean(no_PFsConf_pred_probabilities)).int()\n",
    "\n",
    "# confusion matrix\n",
    "no_PFsConf_matrix = confusion_matrix(actual,predicted, labels=[1,0])\n",
    "print('Confusion matrix : \\n',no_PFsConf_matrix)\n",
    "\n",
    "# outcome values order in sklearn\n",
    "tp, fn, fp, tn = confusion_matrix(actual,predicted,labels=[1,0]).reshape(-1)\n",
    "print('Outcome values : \\n', tp, fn, fp, tn)\n",
    "\n",
    "# classification report for precision, recall f1-score and accuracy\n",
    "no_PFsConf_matrix = classification_report(actual,predicted,labels=[1,0])\n",
    "print('Classification report : \\n',no_PFsConf_matrix)\n",
    "\n",
    "# Calculate Accuracy, Precision, Recall, F1\n",
    "no_PFsConf_accuracy = accuracy_score(test_labels.numpy(), predicted.numpy())\n",
    "no_PFsConf_precision = precision_score(test_labels.numpy(), predicted.numpy())\n",
    "no_PFsConf_recall = recall_score(test_labels.numpy(), predicted.numpy())\n",
    "no_PFsConf_f1 = f1_score(test_labels.numpy(), predicted.numpy())\n",
    "print(f'Accuracy: {no_PFsConf_accuracy}, Precision: {no_PFsConf_precision}, Recall: {no_PFsConf_recall}, F1: {no_PFsConf_f1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b549435-22b0-4895-b53c-b0c2c680400d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1136421-3e42-487d-b788-3fca65789128",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da7aaa0-8314-4035-b0ed-cc24c8a148f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### saving the results\n",
    "\n",
    "results = {\n",
    "    'Metric': ['AUC', 'RS', 'Accuracy', 'Precision', 'Recall', 'F1'],\n",
    "    'Without PFs and Confidence': [no_PFsConf_auc, no_PFsConf_RS, no_PFsConf_accuracy, no_PFsConf_precision, no_PFsConf_recall, no_PFsConf_f1],\n",
    "    'Without Confidence': [no_confidence_auc, no_confidence_RS, no_confidence_accuracy, no_confidence_precision, no_confidence_recall, no_confidence_f1],\n",
    "    'Without PFs': [no_PFs_auc, no_PFs_RS, no_PFs_accuracy, no_PFs_precision, no_PFs_recall, no_PFs_f1],\n",
    "    'TGCN': [ourmodel_auc, ourmodel_RS, ourmodel_accuracy, ourmodel_precision, ourmodel_recall, ourmodel_f1]\n",
    "}\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f4ec864-bf34-4a17-b24b-5a7d12e59bd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6fe118c-e1de-4e86-b5d0-ffd77aa1a0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### saving the results\n",
    "# Create a DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Save to Excel\n",
    "results_df.to_excel('results/withConfusionMatrix/results_Enron_dataset_200epoch.xlsx', index=False)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3daaf2-3ae6-41db-b083-5eac055e9439",
   "metadata": {},
   "source": [
    "### Plotting the results of proposed approach and its variant on bar chart for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5745fc46-9b43-480a-a586-91b149391da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming `results_df` is your DataFrame containing the results\n",
    "rows_to_drop = [\"Accuracy\", \"Precision\", \"Recall\", \"F1\"]\n",
    "\n",
    "# Filter rows where the \"Metric\" column is not in rows_to_drop\n",
    "metrics_filter1 = results_df[~results_df['Metric'].isin(rows_to_drop)]\n",
    "\n",
    "# Metrics and labels\n",
    "metrics = metrics_filter1['Metric']\n",
    "no_PFsConf_values = metrics_filter1['Without PFs and Confidence']\n",
    "no_Conf_values = metrics_filter1['Without Confidence']\n",
    "no_PFs_values = metrics_filter1['Without PFs']\n",
    "TGCN_values = metrics_filter1['TGCN']\n",
    "\n",
    "# Bar width and positions\n",
    "bar_width = 0.15\n",
    "index = range(len(metrics))\n",
    "positions1 = [i - 1.5 * bar_width for i in index]\n",
    "positions2 = [i - 0.5 * bar_width for i in index]\n",
    "positions3 = [i + 0.5 * bar_width for i in index]\n",
    "positions4 = [i + 1.5 * bar_width for i in index]\n",
    "\n",
    "# Colors\n",
    "colors = ['blue', 'green', 'orange', 'purple']\n",
    "\n",
    "# Create bar chart\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "bar1 = ax.bar(positions1, no_PFsConf_values, bar_width, label='Model 1', color=colors[0])\n",
    "bar2 = ax.bar(positions2, no_Conf_values, bar_width, label='Model 2', color=colors[1])\n",
    "bar3 = ax.bar(positions3, no_PFs_values, bar_width, label='Model 3', color=colors[2])\n",
    "bar4 = ax.bar(positions4, TGCN_values, bar_width, label='R-CTGCN', color=colors[3])\n",
    "\n",
    "# Add labels on top of each bar\n",
    "for bars, values in zip([bar1, bar2, bar3, bar4], [no_PFsConf_values, no_Conf_values, no_PFs_values, TGCN_values]):\n",
    "    for bar, value in zip(bars, values):\n",
    "        yval = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2, yval + 0.005, round(value, 3), ha='center', va='bottom')\n",
    "\n",
    "# Adjust other chart elements\n",
    "ax.set_xlabel('Metrics')\n",
    "ax.set_ylabel('Values')\n",
    "ax.set_title('Comparison of Different Model Configurations')\n",
    "ax.set_xticks([i for i in index])\n",
    "ax.set_xticklabels(metrics)\n",
    "ax.legend(loc='upper center', bbox_to_anchor=(0.5, 1.15), ncol=4)\n",
    "\n",
    "# Save the figure\n",
    "plt.savefig('results/withConfusionMatrix/chart_Enron_dataset_200epoch.png', bbox_inches='tight')\n",
    "\n",
    "# Display the chart\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53dff080-fa59-4caa-a0dc-85e6e6c031e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f21364-86e0-4b45-ab39-3d82ef91fd58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "rows_to_drop = [\"AUC\", \"RS\"]\n",
    "# Filter rows where the \"Metric\" column is not in rows_to_drop\n",
    "metrics_filter2 = results_df[~results_df['Metric'].isin(rows_to_drop)]\n",
    "\n",
    "# Metrics and labels\n",
    "metrics = metrics_filter2['Metric']\n",
    "no_PFsConf_values = metrics_filter2['Without PFs and Confidence']\n",
    "no_Conf_values = metrics_filter2['Without Confidence']\n",
    "no_PFs_values = metrics_filter2['Without PFs']\n",
    "TGCN_values = metrics_filter2['TGCN']\n",
    "\n",
    "# Bar width and positions\n",
    "bar_width = 0.2\n",
    "index = range(len(metrics))\n",
    "positions1 = [i - 1.5 * bar_width for i in index]\n",
    "positions2 = [i - 0.5 * bar_width for i in index]\n",
    "positions3 = [i + 0.5 * bar_width for i in index]\n",
    "positions4 = [i + 1.5 * bar_width for i in index]\n",
    "\n",
    "# Colors\n",
    "colors = ['blue', 'green', 'orange', 'purple']\n",
    "\n",
    "# Create bar chart\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "bar1 = ax.bar(positions1, no_PFsConf_values, bar_width, label='Model 1', color=colors[0])\n",
    "bar2 = ax.bar(positions2, no_Conf_values, bar_width, label='Model 2', color=colors[1])\n",
    "bar3 = ax.bar(positions3, no_PFs_values, bar_width, label='Model 3', color=colors[2])\n",
    "bar4 = ax.bar(positions4, TGCN_values, bar_width, label='R-CTGCN', color=colors[3])\n",
    "\n",
    "# Add labels on top of each bar\n",
    "for bars, values in zip([bar1, bar2, bar3, bar4], [no_PFsConf_values, no_Conf_values, no_PFs_values, TGCN_values]):\n",
    "    for bar, value in zip(bars, values):\n",
    "        yval = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2, yval + 0.005, round(value, 3), ha='center', va='bottom')\n",
    "\n",
    "# Adjust other chart elements\n",
    "ax.set_xlabel('Metrics')\n",
    "ax.set_ylabel('Values')\n",
    "ax.set_title('Comparison of Different Model Configurations')\n",
    "ax.set_xticks([i for i in index])\n",
    "ax.set_xticklabels(metrics, rotation=45, ha='right')\n",
    "ax.legend(loc='upper center', bbox_to_anchor=(0.5, 1.15), ncol=4)\n",
    "\n",
    "\n",
    "# Save the figure\n",
    "plt.savefig('results/withConfusionMatrix/Confusionmatrix_chart_Enron_dataset_200epoch.png', bbox_inches='tight')\n",
    "\n",
    "# Display the chart\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35a0794-e74a-4c5a-8d21-8017589ab321",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2058cbee-80cc-4c0e-bdb7-f6c3cde93bed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0f54ee82-2bdf-490c-9003-95f0d6197fa0",
   "metadata": {},
   "source": [
    "# Comparison with Mirror and RGCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979c10d4-39ac-4c58-8bf4-28ec7d32ee43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a random weight matrix with values between 0 and 1\n",
    "random_weight_matrix = np.random.rand(df.shape[0], 1)\n",
    "df['jaccard_similarity'] = df['jaccard_similarity'] / df['jaccard_similarity'].max()\n",
    "df['adamic_similarity'] = df['adamic_similarity'] / df['adamic_similarity'].max()\n",
    "# Combine matrices using horizontal stacking\n",
    "aggregated_matrix = np.hstack([df['jaccard_similarity'].values.reshape(-1, 1), \n",
    "                               df['adamic_similarity'].values.reshape(-1, 1), \n",
    "                               random_weight_matrix])\n",
    "\n",
    "df['aggregated_similarity'] = aggregated_matrix[:, 2]\n",
    "#df['aggregated_similarity'] = df['aggregated_similarity'] * 0.5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89272b8-7cde-48e5-bb2b-a935639b97ce",
   "metadata": {},
   "source": [
    "### Mirror"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54991dfd-a445-421c-b978-f5a5b18b71e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Split the dataset into training and testing sets\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "train_features = torch.tensor(train_df[['jaccard_similarity', 'aggregated_similarity']].values, dtype=torch.float)\n",
    "test_features = torch.tensor(test_df[['jaccard_similarity', 'aggregated_similarity']].values, dtype=torch.float)\n",
    "edge_index_train_np = np.array([train_df['source_index'].values, train_df['target_index'].values])\n",
    "edge_index_test_np = np.array([test_df['source_index'].values, test_df['target_index'].values])\n",
    "edge_index_train = torch.tensor(edge_index_train_np, dtype=torch.long)\n",
    "edge_index_test = torch.tensor(edge_index_test_np, dtype=torch.long)\n",
    "train_labels = torch.tensor(train_df['relation_label'].values, dtype=torch.long)\n",
    "test_labels = torch.tensor(test_df['relation_label'].values, dtype=torch.long)\n",
    "train_data = Data(x=train_features, edge_index=edge_index_train, y=train_labels, edge_type=train_labels)\n",
    "test_data = Data(x=test_features, edge_index=edge_index_test, y=test_labels, edge_type=test_labels)\n",
    "from torch_geometric.nn import RGCNConv\n",
    "# Define the autoencoder model\n",
    "class MirrorAutoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(MirrorAutoencoder, self).__init__()\n",
    "        self.encoder = nn.Linear(input_dim, hidden_dim)\n",
    "        self.decoder = nn.Linear(hidden_dim, input_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = F.relu(self.encoder(x))\n",
    "        decoded = torch.sigmoid(self.decoder(encoded))\n",
    "        return encoded, decoded\n",
    "\n",
    "# Define the TrustRGCNAutoencoder model with the autoencoder\n",
    "class Mirror(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim_ae, hidden_dim_rgcn, output_dim, num_relations, num_bases):\n",
    "        super(Mirror, self).__init__()\n",
    "        self.autoencoder = MirrorAutoencoder(input_dim, hidden_dim_ae)\n",
    "        self.rgcn1 = ChebConv(hidden_dim_ae, hidden_dim_rgcn, K=2, normalization='sym')\n",
    "        self.rgcn2 = RGCNConv(hidden_dim_rgcn, hidden_dim_rgcn, num_relations, num_bases=2)\n",
    "        self.rgcn3 = ChebConv(hidden_dim_rgcn, hidden_dim_rgcn, K=2, normalization='sym')\n",
    "        self.rgcn4 = RGCNConv(hidden_dim_rgcn, output_dim, num_relations, num_bases=2)\n",
    "\n",
    "        \n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_type = data.x, data.edge_index, data.edge_type\n",
    "        encoded, _ = self.autoencoder(x)\n",
    "        x = F.relu(self.rgcn1(encoded, edge_index))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = F.relu(self.rgcn2(x, edge_index, edge_type))\n",
    "        x = F.relu(self.rgcn3(x, edge_index))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.rgcn4(x, edge_index, edge_type)\n",
    "        return torch.sigmoid(x)\n",
    "\n",
    "\n",
    "# Initialize the Mirror method\n",
    "input_dim_ae = train_data.x.shape[1]\n",
    "num_relations = df['relation_label'].nunique()  # Example value\n",
    "output_dim = 1  # Binary classification\n",
    "\n",
    "Mirror_model = Mirror(input_dim_ae, hidden_dim_ae, hidden_dim_rgcn, output_dim, num_relations, num_bases=2)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.BCELoss() # BCELoss  # BCEWithLogitsLoss\n",
    "optimizer = torch.optim.Adam(Mirror_model.parameters(), lr)\n",
    "# .optim.SGD\n",
    "# Train the model\n",
    "Mirror_model.train()\n",
    "for epoch in range(N_Epo):\n",
    "    optimizer.zero_grad()\n",
    "    out = Mirror_model(train_data)\n",
    "    \n",
    "    # Modify target to match the output shape\n",
    "    target = train_data.y.float().view(-1, 1)\n",
    "    \n",
    "    loss = criterion(out, target)\n",
    "    #loss2 = loss * 0.1\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Print training loss for monitoring\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch {epoch}, Loss: {loss.item()}')\n",
    "\n",
    "\n",
    "# Evaluation on test data\n",
    "Mirror_model.eval()\n",
    "with torch.no_grad():\n",
    "    pred = Mirror_model(test_data)\n",
    "\n",
    "\n",
    "# Apply sigmoid to get probability scores\n",
    "Mirrorpred_probabilities = torch.sigmoid(torch.sigmoid(pred))\n",
    "\n",
    "# Compute AUC\n",
    "Mirror_auc = roc_auc_score(test_labels.numpy(), (Mirrorpred_probabilities >= torch.mean(Mirrorpred_probabilities)).int().numpy())\n",
    "\n",
    "# Get the predicted rankings for each edge\n",
    "indices, _ = torch.sort(Mirrorpred_probabilities, descending=True)\n",
    "ranks = torch.zeros_like(indices, dtype=torch.float)  # Initialize ranks\n",
    "\n",
    "# Loop through the sorted indices to calculate ranks\n",
    "for i, idx in enumerate(indices):\n",
    "    ranks[i] = idx + 1  # Adjust rank starting from 1\n",
    "\n",
    "# Number of unlabeled links\n",
    "\n",
    "num_positive_links = torch.sum(test_labels == 1)\n",
    "num_negative_links = torch.sum(test_labels == 0)\n",
    "denominator = (num_positive_links / (num_positive_links - num_negative_links))\n",
    "\n",
    "# Calculate RS for each positive link\n",
    "positive_indices = test_labels == 1\n",
    "positive_ranks = ranks[positive_indices]\n",
    "RSe_values = positive_ranks / denominator\n",
    "\n",
    "Mirror_RS = torch.mean(RSe_values).item()\n",
    "\n",
    "print('Evaluation results for Mirror:')\n",
    "print(f'AUC: {Mirror_auc}, RS: {Mirror_RS}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839add98-d5c9-4ffd-bad4-68af3fc8941f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea514572-384c-47a4-9b28-4f88f6ca4964",
   "metadata": {},
   "outputs": [],
   "source": [
    "# actual values\n",
    "actual = test_labels.numpy()\n",
    "# predicted values\n",
    "predicted = (Mirrorpred_probabilities >= torch.mean(Mirrorpred_probabilities)).int()\n",
    "\n",
    "\n",
    "# confusion matrix\n",
    "Mirror_matrix = confusion_matrix(actual,predicted, labels=[1,0])\n",
    "print('Confusion matrix : \\n',Mirror_matrix)\n",
    "\n",
    "# outcome values order in sklearn\n",
    "tp, fn, fp, tn = confusion_matrix(actual,predicted,labels=[1,0]).reshape(-1)\n",
    "print('Outcome values : \\n', tp, fn, fp, tn)\n",
    "\n",
    "# classification report for precision, recall f1-score and accuracy\n",
    "Mirror_matrix = classification_report(actual,predicted,labels=[1,0])\n",
    "print('Classification report : \\n',Mirror_matrix)\n",
    "\n",
    "# Calculate Accuracy, Precision, Recall, F1\n",
    "Mirror_accuracy = accuracy_score(test_labels.numpy(), predicted.numpy())\n",
    "Mirror_precision = precision_score(test_labels.numpy(), predicted.numpy())\n",
    "Mirror_recall = recall_score(test_labels.numpy(), predicted.numpy())\n",
    "Mirror_f1 = f1_score(test_labels.numpy(), predicted.numpy())\n",
    "print(f'Accuracy: {Mirror_accuracy}, Precision: {Mirror_precision}, Recall: {Mirror_recall}, F1: {Mirror_f1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b6f7ea-e212-4505-ae91-4b37ca072d40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e287edd1-ceaf-42a6-8b95-069433c441d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6f648d23-904c-4ee9-8630-4adc3e8f0952",
   "metadata": {},
   "source": [
    "### RGCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df46c448-0504-4d6f-83a3-6160c26a52fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = torch.tensor(train_df[['jaccard_similarity', 'adamic_similarity']].values, dtype=torch.float)\n",
    "test_features = torch.tensor(test_df[['jaccard_similarity', 'adamic_similarity']].values, dtype=torch.float)\n",
    "edge_index_train_np = np.array([train_df['source_index'].values, train_df['target_index'].values])\n",
    "edge_index_test_np = np.array([test_df['source_index'].values, test_df['target_index'].values])\n",
    "edge_index_train = torch.tensor(edge_index_train_np, dtype=torch.long)\n",
    "edge_index_test = torch.tensor(edge_index_test_np, dtype=torch.long)\n",
    "train_labels = torch.tensor(train_df['relation_label'].values, dtype=torch.long)\n",
    "test_labels = torch.tensor(test_df['relation_label'].values, dtype=torch.long)\n",
    "train_data = Data(x=train_features, edge_index=edge_index_train, y=train_labels, edge_type=train_labels)\n",
    "test_data = Data(x=test_features, edge_index=edge_index_test, y=test_labels, edge_type=test_labels)\n",
    "\n",
    "from torch_geometric.nn import RGCNConv\n",
    "\n",
    "# Define the TrustRGCN model with the autoencoder\n",
    "class RGCN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim_rgcn, output_dim, num_relations):\n",
    "        super(RGCN, self).__init__()\n",
    "        self.rgcn1 = RGCNConv(input_dim, hidden_dim_rgcn, num_relations, num_bases=2)\n",
    "        self.rgcn2 = RGCNConv(hidden_dim_rgcn, hidden_dim_rgcn, num_relations, num_bases=2)\n",
    "        self.rgcn3 = RGCNConv(hidden_dim_rgcn, output_dim, num_relations, num_bases=2)\n",
    "        \n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_type = data.x, data.edge_index, data.edge_type\n",
    "        x = F.normalize(x, p=2, dim=1)\n",
    "        x = F.relu(self.rgcn1(x, edge_index, edge_type))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = F.relu(self.rgcn2(x, edge_index, edge_type))\n",
    "        x = self.rgcn3(x, edge_index, edge_type)\n",
    "        return torch.sigmoid(x)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Initialize the TrustRGCNAutoencoder model\n",
    "input_dim_ae = train_data.x.shape[1]\n",
    "num_relations = df['relation_label'].nunique()  # Example value\n",
    "output_dim = 1  # Binary classification (trust/distrust)\n",
    "\n",
    "RGCNmodel = RGCN(input_dim_ae, hidden_dim_rgcn, output_dim, num_relations)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adadelta(RGCNmodel.parameters(), lr)\n",
    "\n",
    "\n",
    "# Train the model\n",
    "RGCNmodel.train()\n",
    "for epoch in range(N_Epo):  # You can adjust the number of epochs\n",
    "    optimizer.zero_grad()\n",
    "    out = RGCNmodel(train_data)\n",
    "    target = train_data.y.float().view(-1, 1)\n",
    "    loss = criterion(out, target)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Print training loss for monitoring\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch {epoch}, Loss: {loss.item()}')\n",
    "\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "RGCNmodel.eval()\n",
    "with torch.no_grad():\n",
    "    pred_rgcn = RGCNmodel(test_data)\n",
    "\n",
    "# Apply sigmoid to get probability scores\n",
    "pred_rgcn_probabilities = torch.sigmoid(pred_rgcn)\n",
    "\n",
    "# Compute AUC\n",
    "RGCN_auc = roc_auc_score(test_labels.numpy(), (pred_rgcn_probabilities >= torch.mean(pred_rgcn_probabilities)).int().numpy())\n",
    "\n",
    "\n",
    "# Get the predicted rankings for each edge\n",
    "indices, _ = torch.sort(pred_rgcn_probabilities, descending=True)\n",
    "ranks = torch.zeros_like(indices, dtype=torch.float)  # Initialize ranks\n",
    "\n",
    "# Loop through the sorted indices to calculate ranks\n",
    "for i, idx in enumerate(indices):\n",
    "    ranks[i] = idx + 1  # Adjust rank starting from 1\n",
    "\n",
    "# Number of unlabeled links\n",
    "num_positive_links = torch.sum(test_labels == 1)\n",
    "num_negative_links = torch.sum(test_labels == 0)\n",
    "denominator = (num_positive_links / (num_positive_links - num_negative_links))\n",
    "\n",
    "# Calculate RS for each positive link\n",
    "positive_indices = test_labels == 1\n",
    "positive_ranks = ranks[positive_indices]\n",
    "RSe_values = positive_ranks / denominator\n",
    "RGCN_RS = torch.mean(RSe_values).item()\n",
    "\n",
    "print('Evaluation results for R-GCN:')\n",
    "print(f'AUC: {RGCN_auc}, RS: {RGCN_RS}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9e5d16-a35f-41e7-a33c-d4314a428033",
   "metadata": {},
   "outputs": [],
   "source": [
    "# actual values\n",
    "actual = test_labels.numpy()\n",
    "# predicted values\n",
    "predicted = (pred_rgcn_probabilities >= torch.mean(pred_rgcn_probabilities)).int()\n",
    "\n",
    "\n",
    "# confusion matrix\n",
    "RGCN_matrix = confusion_matrix(actual,predicted, labels=[1,0])\n",
    "print('Confusion matrix : \\n',RGCN_matrix)\n",
    "\n",
    "# outcome values order in sklearn\n",
    "tp, fn, fp, tn = confusion_matrix(actual,predicted,labels=[1,0]).reshape(-1)\n",
    "print('Outcome values : \\n', tp, fn, fp, tn)\n",
    "\n",
    "# classification report for precision, recall f1-score and accuracy\n",
    "RGCN_matrix = classification_report(actual,predicted,labels=[1,0])\n",
    "print('Classification report : \\n',RGCN_matrix)\n",
    "\n",
    "# Calculate Accuracy, Precision, Recall, F1\n",
    "RGCN_accuracy = accuracy_score(test_labels.numpy(), predicted.numpy())\n",
    "RGCN_precision = precision_score(test_labels.numpy(), predicted.numpy())\n",
    "RGCN_recall = recall_score(test_labels.numpy(), predicted.numpy())\n",
    "RGCN_f1 = f1_score(test_labels.numpy(), predicted.numpy())\n",
    "print(f'Accuracy: {RGCN_accuracy}, Precision: {RGCN_precision}, Recall: {RGCN_recall}, F1: {RGCN_f1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc32acb-6026-4c2d-8440-ed03b0b9aae2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071efab2-ca48-48c4-a086-c11cca1caaf8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "37f0bf9f-e3cb-41e8-88a6-f694209f6a32",
   "metadata": {},
   "source": [
    "### Add resutls of Mirror and RGCN to the results df and resave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5820db-81e7-46c0-b5ed-6b9e2ae9b054",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df['Mirror'] = [Mirror_auc, Mirror_RS, Mirror_accuracy, Mirror_precision, Mirror_recall, Mirror_f1]\n",
    "results_df['R-GCN'] = [RGCN_auc, RGCN_RS, RGCN_accuracy, RGCN_precision, RGCN_recall, RGCN_f1]\n",
    "\n",
    "# Save new one to Excel\n",
    "results_df.to_excel('results/withConfusionMatrix/results_Enron_dataset_200epoch.xlsx', index=False)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3538c2d6-8533-4198-a6d8-09bdac0a478a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3436422e-a2f2-4cba-bd27-364c26a0740a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_to_drop = [\"Accuracy\", \"Precision\", \"Recall\", \"F1\"]\n",
    "\n",
    "# Filter rows where the \"Metric\" column is not in rows_to_drop\n",
    "metrics_filter1 = results_df[~results_df['Metric'].isin(rows_to_drop)]\n",
    "metrics_filter1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbbf2e10-f019-4c9b-8da2-5890753bc6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_to_drop = [\"AUC\", \"RS\"]\n",
    "# Filter rows where the \"Metric\" column is not in rows_to_drop\n",
    "metrics_filter2 = results_df[~results_df['Metric'].isin(rows_to_drop)]\n",
    "metrics_filter2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b780abfa-0e96-4b9d-8ed2-718759d7815d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca25f2cc-6b5c-49a8-9366-39f6e8bfd4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Metrics and labels\n",
    "metrics = metrics_filter1['Metric']\n",
    "RCTGCN_values = metrics_filter1['TGCN']\n",
    "RGCN_values = metrics_filter1['R-GCN']\n",
    "Mirror_values = metrics_filter1['Mirror']\n",
    "\n",
    "# Bar width and positions\n",
    "bar_width = 0.2\n",
    "index = range(len(metrics))\n",
    "positions1 = [i - 1.1 * bar_width for i in index]\n",
    "positions2 = [i for i in index]\n",
    "positions3 = [i + 1.1 * bar_width for i in index]\n",
    "\n",
    "# Colors\n",
    "colors = ['blue', 'green', 'orange', 'purple', 'maroon', 'cyan']\n",
    "\n",
    "# Create bar chart\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "bar1 = ax.bar(positions1, RCTGCN_values, bar_width, label='R-CTGCN', color=colors[3])\n",
    "bar2 = ax.bar(positions2, RGCN_values, bar_width, label='R-GCN', color=colors[4])\n",
    "bar3 = ax.bar(positions3, Mirror_values, bar_width, label='Mirror', color=colors[5])\n",
    "\n",
    "# Add labels on top of each bar\n",
    "all_bars = [bar1, bar2, bar3]\n",
    "all_values = [RCTGCN_values, RGCN_values, Mirror_values]\n",
    "for bars, values in zip(all_bars, all_values):\n",
    "    for bar, value in zip(bars, values):\n",
    "        yval = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2, yval + 0.005, round(value, 3), ha='center', va='bottom')\n",
    "\n",
    "# Adjust other chart elements\n",
    "ax.set_xlabel('Metrics')\n",
    "ax.set_ylabel('Values')\n",
    "ax.set_title('Comparison of Different Models')\n",
    "ax.set_xticks([i for i in index])\n",
    "ax.set_xticklabels(metrics)\n",
    "ax.legend(loc='upper center', bbox_to_anchor=(0.5, 1.15), ncol=6)\n",
    "\n",
    "# Save the new figure\n",
    "plt.savefig('results/withConfusionMatrix/02_chart_Enron_data_200epoch_2.png', bbox_inches='tight')\n",
    "\n",
    "# Display the chart\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469ad574-8755-44bc-9fb8-8dbc4be3e284",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa0aa4c-01b9-481b-9b59-5bcb03619d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Metrics and labels\n",
    "metrics = metrics_filter2['Metric']\n",
    "RCTGCN_values = metrics_filter2['TGCN']\n",
    "RGCN_values = metrics_filter2['R-GCN']\n",
    "Mirror_values = metrics_filter2['Mirror']\n",
    "\n",
    "# Bar width and positions\n",
    "bar_width = 0.2\n",
    "index = range(len(metrics))\n",
    "positions1 = [i - 1.1 * bar_width for i in index]\n",
    "positions2 = [i for i in index]\n",
    "positions3 = [i + 1.1 * bar_width for i in index]\n",
    "\n",
    "# Colors\n",
    "colors = ['purple', 'maroon', 'cyan']\n",
    "\n",
    "# Create bar chart\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "bar1 = ax.bar(positions1, RCTGCN_values, bar_width, label='R-CTGCN', color=colors[0])\n",
    "bar2 = ax.bar(positions2, RGCN_values, bar_width, label='R-GCN', color=colors[1])\n",
    "bar3 = ax.bar(positions3, Mirror_values, bar_width, label='Mirror', color=colors[2])\n",
    "\n",
    "# Add labels on top of each bar\n",
    "all_bars = [bar1, bar2, bar3]\n",
    "all_values = [RCTGCN_values, RGCN_values, Mirror_values]\n",
    "for bars, values in zip(all_bars, all_values):\n",
    "    for bar, value in zip(bars, values):\n",
    "        yval = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2, yval + 0.005, round(value, 3), ha='center', va='bottom')\n",
    "\n",
    "# Adjust other chart elements\n",
    "ax.set_xlabel('Metrics')\n",
    "ax.set_ylabel('Values')\n",
    "ax.set_title('Comparison of Different Models')\n",
    "ax.set_xticks([i for i in index])\n",
    "ax.set_xticklabels(metrics)\n",
    "ax.legend(loc='upper center', bbox_to_anchor=(0.5, 1.15), ncol=6)\n",
    "\n",
    "# Save the new figure\n",
    "plt.savefig('results/withConfusionMatrix/02_Confusionmatrix_chart_Enron_data_200epoch_2.png', bbox_inches='tight')\n",
    "\n",
    "# Display the chart\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69345fb6-5df7-40e2-a671-20c1f79e8fd0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cffb6f90-225b-4104-97ec-2e04cf1c5f36",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
